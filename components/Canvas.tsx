'use client';

import { useCallback, useMemo, useEffect, useState, useRef } from 'react';
import {
  ReactFlow,
  ReactFlowProvider,
  Node,
  Edge,
  Background,
  BackgroundVariant,
  useNodesState,
  useEdgesState,
  NodeTypes,
  SelectionMode,
  OnSelectionChangeParams,
  ConnectionLineType,
  OnConnectStart,
  OnConnectEnd,
  useReactFlow,
  type Connection,
} from '@xyflow/react';
import '@xyflow/react/dist/style.css';

import { toast } from 'sonner';
import { useCanvasStore } from '@/lib/store';
import ImageNode from './nodes/ImageNode';
import TextNode from './nodes/TextNode';
import VideoNode from './nodes/VideoNode';
import NoteNode from './nodes/NoteNode';
import CanvasNavigation from './CanvasNavigation';
import RightToolbar from './RightToolbar';
import AIInputPanel from './AIInputPanel';
import Toolbar from './Toolbar';
import SelectionToolbar from './SelectionToolbar';
import ConnectionMenuRoot from './canvas/connection-menu/ConnectionMenuRoot';
import ImageAnnotatorModal, { ImageAnnotatorResult } from './ImageAnnotatorModal';
import ThemeToggle from './ThemeToggle';
import { useThemeStore } from '@/lib/theme-store';
import { CanvasElement, VideoElement, ImageElement, TextElement, NoteElement, ReshootMotionType } from '@/lib/types';
import { generateVideoFromText, generateVideoFromImages, generateImage, imageToImage, registerUploadedImage } from '@/lib/api-mock';
import { loadMaterialsFromProject } from '@/lib/project-materials';
import {
  getPositionAboveInput,
  generateFromInput,
  imageToImageFromInput,
  multiImageRecipeFromInput,
} from '@/lib/input-panel-generator';
import { useConnectionMenu } from '@/hooks/canvas/useConnectionMenu';
import { ConnectionMenuCallbacks } from '@/types/connection-menu';
import { useTextToImage } from '@/hooks/canvas/useTextToImage';
import { useImageToImage } from '@/hooks/canvas/useImageToImage';
import { ImageAspectRatio } from '@/types/image-generation';
import {
  VIDEO_NODE_DEFAULT_SIZE,
  IMAGE_NODE_DEFAULT_SIZE,
  TEXT_NODE_DEFAULT_SIZE,
  getVideoNodeSize,
  getImageNodeSize,
} from '@/lib/constants/node-sizes';

// æ³¨å†Œè‡ªå®šä¹‰èŠ‚ç‚¹ç±»å‹
const nodeTypes: NodeTypes = {
  image: ImageNode,
  text: TextNode,
  video: VideoNode,
  note: NoteNode, // è¡Œçº§æ³¨é‡Šï¼šè®°äº‹æœ¬èŠ‚ç‚¹ï¼ˆå‰§æœ¬ã€åˆ†é•œç­‰é•¿æ–‡æœ¬ï¼‰
};

const EDGE_DEFAULT_STYLE = { stroke: '#64748b', strokeWidth: 1 };

// è¡Œçº§æ³¨é‡Šï¼šä½¿ç”¨ VL æ¨¡å‹åˆ†æå›¾ç‰‡ç”Ÿæˆè§†é¢‘æç¤ºè¯
async function analyzeImageForVideoPrompt(
  imageUrl: string,
  endImageUrl: string | null,
  dashScopeApiKey: string
): Promise<string> {
  const isStartEndMode = Boolean(endImageUrl);
  
  // è¡Œçº§æ³¨é‡Šï¼š8ç§’è§†é¢‘ï¼Œéœ€è¦ 1-2 ä¸ªé•œå¤´åˆ‡æ¢ï¼Œæ¯ä¸ªé•œå¤´ 2-3 ç§’
  const systemPrompt = isStartEndMode
    ? `Analyze these two images (start frame and end frame) and generate an 8-second video prompt.

STRUCTURE: Design 2-3 shots (each 2-3 seconds) that transition from Frame A to Frame B:
- Shot 1 (0-3s): Starting action/camera from Frame A
- Shot 2 (3-6s): Transition movement, camera change, or mid-action
- Shot 3 (6-8s): Arriving at Frame B's composition

Include: character movement, camera cuts/pans, environmental changes, mood shifts.
Output ONLY the prompt text describing all shots in sequence. Under 80 words. English.`
    : `Analyze this image and generate an 8-second cinematic video prompt.

STRUCTURE: Design 2-3 shots (each 2-3 seconds):
- Shot 1 (0-3s): Initial scene, subtle movement begins
- Shot 2 (3-6s): Camera change or new action (cut to different angle, pan, or zoom)
- Shot 3 (6-8s): Concluding motion or reveal

Include: character actions, camera movements (pan/zoom/cut), environmental motion.
Output ONLY the prompt text describing all shots in sequence. Under 80 words. English.`;

  const messages: any[] = [{
    role: 'user',
    content: isStartEndMode
      ? [
          { type: 'image_url', image_url: { url: imageUrl } },
          { type: 'image_url', image_url: { url: endImageUrl! } },
          { type: 'text', text: systemPrompt }
        ]
      : [
          { type: 'image_url', image_url: { url: imageUrl } },
          { type: 'text', text: systemPrompt }
        ]
  }];

  const response = await fetch('https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${dashScopeApiKey}`
    },
    body: JSON.stringify({
      model: 'qwen-vl-max',
      messages
    })
  });

  if (!response.ok) {
    const err = await response.json();
    throw new Error(err.error?.message || 'VL API request failed');
  }

  const data = await response.json();
  const content = data.choices[0]?.message?.content || '';
  
  // æ¸…ç†è¿”å›å†…å®¹
  return content.trim().replace(/^["']|["']$/g, '');
}

function CanvasContent({ projectId }: { projectId?: string }) {
  const elements = useCanvasStore((state) => state.elements);
  const updateElement = useCanvasStore((state) => state.updateElement);
  const addElement = useCanvasStore((state) => state.addElement);
  const setSelection = useCanvasStore((state) => state.setSelection);
  const uiState = useCanvasStore((state) => state.uiState);
  const loadProjectPrefixPrompt = useCanvasStore((state) => state.loadProjectPrefixPrompt);
  
  // ä¸»é¢˜çŠ¶æ€
  const theme = useThemeStore((state) => state.theme);

  // å›¾ç‰‡ç¼–è¾‘å™¨çŠ¶æ€ - ä½¿ç”¨å“åº”å¼ hooks
  const annotatorTarget = useCanvasStore((state) => state.annotatorTarget);
  const isLoadingAnnotatorImage = useCanvasStore((state) => state.isLoadingAnnotatorImage);
  const setAnnotatorTarget = useCanvasStore((state) => state.setAnnotatorTarget);
  const setIsLoadingAnnotatorImage = useCanvasStore((state) => state.setIsLoadingAnnotatorImage);

  // è¡Œçº§æ³¨é‡Šï¼šå¤šå›¾ç¼–è¾‘ - ä¸»å›¾å’Œå‚è€ƒå›¾
  const [mainImageForEdit, setMainImageForEdit] = useState<ImageElement | null>(null);
  const [referenceImages, setReferenceImages] = useState<ImageElement[]>([]);

  // è¡Œçº§æ³¨é‡Šï¼šReact Flow èŠ‚ç‚¹å’Œè¾¹ç¼˜çŠ¶æ€ï¼ˆéœ€è¦åœ¨ Hooks ä¹‹å‰å£°æ˜ï¼‰
  const [reactFlowNodes, setNodes, onNodesChange] = useNodesState(elements.map(el => ({
    id: el.id,
    type: el.type,
    position: el.position,
    data: el as any,
    draggable: true,
    style: el.size ? {
      width: el.size.width,
      height: el.size.height,
    } : undefined,
  })));
  const [edges, setEdges, onEdgesChange] = useEdgesState<Edge>([]);

  const reactFlowInstance = useReactFlow();

  // è¡Œçº§æ³¨é‡Šï¼šä½¿ç”¨è¿çº¿èœå• Hook ç®¡ç†èœå•çŠ¶æ€
  const {
    connectionMenu,
    promptMenuInputRef,
    resetConnectionMenu,
    showConnectionMenu,
    showImageSubmenu,
    showVideoSubmenu,
    showImagePromptInput,
    updateImagePrompt,
    backToMain,
    backToImageSubmenu,
    prepareConnectionMenu,
    showCameraControlSubmenu,
    showCameraPositionSubmenu,
    showCustomNextShotInput,
    showAutoNextShotCountSubmenu,
  } = useConnectionMenu();

  // è¡Œçº§æ³¨é‡Šï¼šä½¿ç”¨å›¾ç‰‡ç”Ÿæˆ Hooks
  const { handleTextToImage } = useTextToImage({
    addElement,
    updateElement,
    setEdges,
    resetConnectionMenu,
  });

  const { handleImageToImage } = useImageToImage({
    addElement,
    setEdges,
    resetConnectionMenu,
  });

  // è¡Œçº§æ³¨é‡Šï¼šåŒæ­¥ elements åˆ° React Flow èŠ‚ç‚¹çŠ¶æ€ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šåªåœ¨å…ƒç´ æ•°é‡æˆ– ID å˜åŒ–æ—¶å®Œå…¨é‡å»ºï¼‰
  const elementsIdsRef = useRef<string>('');
  const previousElementsRef = useRef<CanvasElement[]>(elements);

  useEffect(() => {
    const newIdsString = elements.map(el => el.id).sort().join(',');

    // è¡Œçº§æ³¨é‡Šï¼šåªæœ‰å…ƒç´ æ•°é‡/ID å˜åŒ–æ—¶æ‰å®Œå…¨é‡å»ºèŠ‚ç‚¹åˆ—è¡¨ï¼ˆæ–°å¢/åˆ é™¤èŠ‚ç‚¹ï¼‰
    if (elementsIdsRef.current !== newIdsString) {
      elementsIdsRef.current = newIdsString;
      setNodes(elements.map(el => ({
        id: el.id,
        type: el.type,
        position: el.position,
        data: el as any,
        draggable: true,
        style: el.size ? {
          width: el.size.width,
          height: el.size.height,
        } : undefined,
      })));
      previousElementsRef.current = elements;
    } else {
      // è¡Œçº§æ³¨é‡Šï¼šå…ƒç´ æ•°é‡ä¸å˜æ—¶ï¼Œåªæ›´æ–°èŠ‚ç‚¹çš„ data å’Œ styleï¼ˆé¿å…é‡å»ºæ•´ä¸ªåˆ—è¡¨ï¼‰
      setNodes((currentNodes) =>
        currentNodes.map((node) => {
          const element = elements.find((el) => el.id === node.id);
          const prevElement = previousElementsRef.current.find((el) => el.id === node.id);
          if (!element) return node;

          // è¡Œçº§æ³¨é‡Šï¼šæ£€æŸ¥ position æ˜¯å¦çœŸçš„å˜åŒ–äº†ï¼Œé¿å…ä¸å¿…è¦çš„ä½ç½®æ›´æ–°ï¼ˆä¿®å¤è§†é¢‘ç”Ÿæˆæ—¶çš„è·³åŠ¨é—®é¢˜ï¼‰
          const positionChanged = !prevElement ||
            prevElement.position.x !== element.position.x ||
            prevElement.position.y !== element.position.y;

          return {
            ...node,
            data: element as any,
            // è¡Œçº§æ³¨é‡Šï¼šåªåœ¨ä½ç½®çœŸçš„å˜åŒ–æ—¶æ‰æ›´æ–° positionï¼Œå¦åˆ™ä¿æŒ React Flow å†…éƒ¨çš„ä½ç½®ä¸å˜
            position: positionChanged ? element.position : node.position,
            style: element.size ? {
              width: element.size.width,
              height: element.size.height,
            } : undefined,
          };
        })
      );
      previousElementsRef.current = elements;
    }
  }, [elements, setNodes]);

  useEffect(() => {
    if (!projectId) {
      return;
    }
    // è®¾ç½® projectId åˆ° store çš„ apiConfig ä¸­
    useCanvasStore.setState((state) => ({
      apiConfig: {
        ...state.apiConfig,
        projectId,
      },
    }));
    // åŠ è½½é¡¹ç›®çš„å‰ç½®æç¤ºè¯
    loadProjectPrefixPrompt(projectId);
    // è¡Œçº§æ³¨é‡Šï¼šç´ æåº“æ”¹ä¸ºæ‰‹åŠ¨åŠ è½½ï¼Œä¸è‡ªåŠ¨åŠ è½½
  }, [projectId, loadProjectPrefixPrompt]);

  const reactFlowWrapperRef = useRef<HTMLDivElement | null>(null);
  const connectionStartRef = useRef<{
    sourceId: string;
    sourceType: CanvasElement['type'];
    handleId?: string | null;
    didConnect: boolean;
  } | null>(null);
  const activeGenerationRef = useRef<Set<string>>(new Set());

  const maybeStartVideo = useCallback(
    async (videoId: string) => {
      if (activeGenerationRef.current.has(videoId)) {
        return;
      }

      const { elements: storeElements } = useCanvasStore.getState();
      const videoElement = storeElements.find((el) => el.id === videoId) as VideoElement | undefined;

      if (!videoElement) return;

      if (videoElement.status !== 'queued') {
        return;
      }

      let promptText = videoElement.promptText?.trim();
      const startImageId = videoElement.startImageId;
      const endImageId = videoElement.endImageId;
      const generationCount = videoElement.generationCount || 1; // è¡Œçº§æ³¨é‡Šï¼šè·å–ç”Ÿæˆæ•°é‡

      const hasAtLeastOneImage = Boolean(startImageId || endImageId);
      
      // è¡Œçº§æ³¨é‡Šï¼šæ™ºèƒ½è§†é¢‘ç”Ÿæˆ - å¦‚æœæœ‰å›¾ç‰‡ä½†æ²¡æœ‰æç¤ºè¯ï¼Œä½¿ç”¨ VL åˆ†æç”Ÿæˆæç¤ºè¯
      if (hasAtLeastOneImage && !promptText) {
        const { apiConfig } = useCanvasStore.getState();
        const dashScopeApiKey = apiConfig.dashScopeApiKey;
        if (!dashScopeApiKey) {
          console.warn('âš ï¸ æ²¡æœ‰é…ç½® DashScope API Keyï¼Œæ— æ³•ä½¿ç”¨æ™ºèƒ½åˆ†æ');
          updateElement(videoId, {
            status: 'pending',
            readyForGeneration: false,
          } as Partial<VideoElement>);
          return;
        }

        // è¡Œçº§æ³¨é‡Šï¼šè·å–é¦–å¸§å›¾ç‰‡ä¿¡æ¯
        const startImage = startImageId 
          ? storeElements.find(el => el.id === startImageId) as ImageElement | undefined
          : null;
        const endImage = endImageId 
          ? storeElements.find(el => el.id === endImageId) as ImageElement | undefined
          : null;
        
        const actualStartImage = startImage || endImage;
        if (!actualStartImage?.src) {
          console.warn('âš ï¸ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„å›¾ç‰‡æº');
          updateElement(videoId, {
            status: 'pending',
            readyForGeneration: false,
          } as Partial<VideoElement>);
          return;
        }

        try {
          updateElement(videoId, {
            status: 'generating',
            progress: 5,
          } as Partial<VideoElement>);

          console.log('ğŸ” ä½¿ç”¨ VL åˆ†æå›¾ç‰‡ç”Ÿæˆè§†é¢‘æç¤ºè¯...');
          
          // è¡Œçº§æ³¨é‡Šï¼šè·å–é¦–å¸§å›¾ç‰‡æ•°æ®ï¼ˆä¸è‡ªåŠ¨åˆ†é•œé€»è¾‘ä¸€è‡´ï¼‰
          let startImageData = actualStartImage.src;
          if (actualStartImage.base64) {
            startImageData = actualStartImage.base64.startsWith('data:') 
              ? actualStartImage.base64 
              : `data:image/png;base64,${actualStartImage.base64}`;
          }
          
          // è¡Œçº§æ³¨é‡Šï¼šè·å–å°¾å¸§å›¾ç‰‡æ•°æ®
          let endImageData: string | null = null;
          if (startImage && endImage && endImage.id !== startImage.id) {
            if (endImage.base64) {
              endImageData = endImage.base64.startsWith('data:') 
                ? endImage.base64 
                : `data:image/png;base64,${endImage.base64}`;
            } else if (endImage.src) {
              endImageData = endImage.src;
            }
          }

          promptText = await analyzeImageForVideoPrompt(startImageData, endImageData, dashScopeApiKey);
          console.log('âœ… VL åˆ†æå®Œæˆï¼Œç”Ÿæˆæç¤ºè¯:', promptText);
          
          // è¡Œçº§æ³¨é‡Šï¼šæ›´æ–°è§†é¢‘èŠ‚ç‚¹çš„æç¤ºè¯
          updateElement(videoId, {
            promptText: promptText,
            progress: 15,
          } as Partial<VideoElement>);
        } catch (error) {
          console.error('âŒ VL åˆ†æå¤±è´¥:', error);
          updateElement(videoId, {
            status: 'error',
            readyForGeneration: false,
          } as Partial<VideoElement>);
          return;
        }
      }
      
      // è¡Œçº§æ³¨é‡Šï¼šæ”¯æŒçº¯æ–‡æœ¬ç”Ÿæˆè§†é¢‘ - åªè¦æœ‰æç¤ºè¯å°±å¯ä»¥ç”Ÿæˆ
      const ready = Boolean(promptText);

      if (!ready) {
        updateElement(videoId, {
          status: 'pending',
          readyForGeneration: ready,
        } as Partial<VideoElement>);
        return;
      }

      console.log('ğŸ¬ maybeStartVideo: å¼€å§‹ç”Ÿæˆè§†é¢‘', { videoId, generationCount, promptText });

      // è¡Œçº§æ³¨é‡Šï¼šå¦‚æœ generationCount > 1ï¼Œåˆ›å»ºé¢å¤–çš„è§†é¢‘èŠ‚ç‚¹
      if (generationCount > 1) {
        const basePosition = videoElement.position;
        const size = videoElement.size || VIDEO_NODE_DEFAULT_SIZE;
        const spacing = 50; // è¡Œçº§æ³¨é‡Šï¼šèŠ‚ç‚¹ä¹‹é—´çš„é—´è·

        for (let i = 1; i < generationCount; i++) {
          const newVideoId = `video-${Date.now()}-${i}`;
          const newPosition = {
            x: basePosition.x + (size.width + spacing) * i,
            y: basePosition.y,
          };

          const newVideo: VideoElement = {
            id: newVideoId,
            type: 'video',
            src: '',
            thumbnail: '',
            duration: 0,
            status: 'queued',
            progress: 0,
            position: newPosition,
            size: size,
            promptText: promptText,
            startImageId: startImageId,
            endImageId: endImageId,
            generationCount: 1, // è¡Œçº§æ³¨é‡Šï¼šæ¯ä¸ªèŠ‚ç‚¹åªç”Ÿæˆä¸€ä¸ªè§†é¢‘
            generatedFrom: videoElement.generatedFrom,
          };

          const addElement = useCanvasStore.getState().addElement;
          addElement(newVideo);

          // è¡Œçº§æ³¨é‡Šï¼šåˆ›å»ºè¿çº¿
          if (videoElement.generatedFrom?.type === 'extend' || videoElement.generatedFrom?.type === 'reshoot') {
            // è¡Œçº§æ³¨é‡Šï¼šå»¶é•¿/é‡æ‹è§†é¢‘ - è¿æ¥åˆ°æºè§†é¢‘èŠ‚ç‚¹
            const sourceVideoId = videoElement.generatedFrom.sourceIds[0];
            if (sourceVideoId) {
              const edgeId = `edge-${sourceVideoId}-${newVideoId}-${videoElement.generatedFrom.type}`;
              setEdges((eds: any[]) => [
                ...eds,
                {
                  id: edgeId,
                  source: sourceVideoId,
                  target: newVideoId,
                  type: 'default',
                  animated: true,
                  style: { stroke: '#a855f7', strokeWidth: 1 },
                  label: videoElement.generatedFrom?.type === 'extend' ? 'å»¶é•¿' : 'é•œå¤´æ§åˆ¶',
                },
              ]);
            }
          } else {
            // è¡Œçº§æ³¨é‡Šï¼šå›¾ç”Ÿè§†é¢‘ - è¿æ¥åˆ°å›¾ç‰‡èŠ‚ç‚¹
            if (startImageId) {
              const edgeId = `edge-${startImageId}-${newVideoId}-start`;
              setEdges((eds: any[]) => [
                ...eds,
                {
                  id: edgeId,
                  source: startImageId,
                  sourceHandle: null,
                  target: newVideoId,
                  targetHandle: 'start-image',
                  type: 'default',
                  animated: true,
                  style: { stroke: '#3b82f6', strokeWidth: 2 },
                },
              ]);
            }
            if (endImageId && endImageId !== startImageId) {
              const edgeId = `edge-${endImageId}-${newVideoId}-end`;
              setEdges((eds: any[]) => [
                ...eds,
                {
                  id: edgeId,
                  source: endImageId,
                  sourceHandle: null,
                  target: newVideoId,
                  targetHandle: 'end-image',
                  type: 'default',
                  animated: true,
                  style: { stroke: '#3b82f6', strokeWidth: 2 },
                },
              ]);
            }
          }

          console.log('âœ… åˆ›å»ºé¢å¤–è§†é¢‘èŠ‚ç‚¹:', newVideoId);

          // è¡Œçº§æ³¨é‡Šï¼šå»¶è¿Ÿè§¦å‘ç”Ÿæˆï¼Œé¿å…åŒæ—¶å‘èµ·å¤ªå¤šè¯·æ±‚
          setTimeout(() => {
            maybeStartVideo(newVideoId);
          }, i * 500); // æ¯ä¸ªè§†é¢‘é—´éš” 0.5 ç§’
        }
      }

      activeGenerationRef.current.add(videoId);

      updateElement(videoId, {
        status: 'generating',
        readyForGeneration: true,
        src: '',
        thumbnail: '',
      } as Partial<VideoElement>);

      // @ts-ignore
      setEdges((eds: any[]) =>
        eds.map((edge: any) =>
          edge.target === videoId
            ? {
              ...edge,
              animated: true,
              style: { stroke: '#a855f7', strokeWidth: 1 },
            }
            : edge
        )
      );

      try {
        let result;
        let generationType: 'text-to-video' | 'image-to-image' | 'extend' | 'reshoot' = 'text-to-video';
        const combinedSourceIds = new Set<string>(videoElement.generatedFrom?.sourceIds ?? []);

        // è¡Œçº§æ³¨é‡Šï¼šåˆ¤æ–­è§†é¢‘ç±»å‹å¹¶è°ƒç”¨å¯¹åº” API
        if (videoElement.generatedFrom?.type === 'extend') {
          // è¡Œçº§æ³¨é‡Šï¼šå»¶é•¿è§†é¢‘
          const sourceVideoId = videoElement.generatedFrom.sourceIds[0];
          if (!sourceVideoId) {
            throw new Error('ç¼ºå°‘æºè§†é¢‘èŠ‚ç‚¹ID');
          }

          const sourceVideo = storeElements.find(el => el.id === sourceVideoId) as VideoElement | undefined;
          if (!sourceVideo || !sourceVideo.mediaGenerationId) {
            throw new Error('æºè§†é¢‘ç¼ºå°‘ mediaGenerationId');
          }

          const aspectRatio = videoElement.size?.width && videoElement.size?.height
            ? (Math.abs(videoElement.size.width / videoElement.size.height - 16 / 9) < 0.1 ? '16:9'
              : Math.abs(videoElement.size.width / videoElement.size.height - 1) < 0.1 ? '1:1'
                : '9:16')
            : '16:9';

          const { generateVideoExtend } = await import('@/lib/api-mock');
          result = await generateVideoExtend(
            sourceVideo.mediaGenerationId,
            promptText || '',
            aspectRatio as any
          );
          generationType = 'extend';
        } else if (videoElement.generatedFrom?.type === 'reshoot') {
          // è¡Œçº§æ³¨é‡Šï¼šé•œå¤´æ§åˆ¶é‡æ‹ï¼ˆå·²åœ¨å…¶ä»–åœ°æ–¹å¤„ç†ï¼Œè¿™é‡Œä¸åº”è¯¥è¿›å…¥ï¼‰
          console.warn('âš ï¸ Reshoot è§†é¢‘ä¸åº”è¯¥é€šè¿‡ maybeStartVideo ç”Ÿæˆ');
          return;
        } else if (hasAtLeastOneImage) {
          // è¡Œçº§æ³¨é‡Šï¼šå›¾ç”Ÿè§†é¢‘ - ä½¿ç”¨é¦–å°¾å¸§
          const actualStartId = startImageId || endImageId!;
          const actualEndId = startImageId && endImageId ? endImageId : undefined;

          result = await generateVideoFromImages(actualStartId, actualEndId, promptText);

          if (startImageId) combinedSourceIds.add(startImageId);
          if (endImageId) combinedSourceIds.add(endImageId);
          generationType = 'image-to-image';
        } else {
          // è¡Œçº§æ³¨é‡Šï¼šçº¯æ–‡æœ¬ç”Ÿæˆè§†é¢‘
          const aspectRatio = videoElement.size?.width && videoElement.size?.height
            ? (Math.abs(videoElement.size.width / videoElement.size.height - 16 / 9) < 0.1 ? '16:9'
              : Math.abs(videoElement.size.width / videoElement.size.height - 9 / 16) < 0.1 ? '9:16'
                : '1:1')
            : '9:16'; // è¡Œçº§æ³¨é‡Šï¼šé»˜è®¤ç«–å±ï¼ˆä¸ Google å®˜æ–¹é»˜è®¤ä¸€è‡´ï¼‰

          console.log('ğŸ¬ è°ƒç”¨æ–‡ç”Ÿè§†é¢‘:', { promptText, aspectRatio });
          result = await generateVideoFromText(promptText || '', aspectRatio as '16:9' | '9:16' | '1:1');
          generationType = 'text-to-video';
        }

        updateElement(videoId, {
          status: 'ready',
          src: result.videoUrl,
          thumbnail: result.thumbnail,
          duration: result.duration,
          mediaGenerationId: result.mediaGenerationId,
          progress: 100,
          readyForGeneration: true,
          generatedFrom: {
            type: generationType,
            sourceIds: Array.from(combinedSourceIds),
            prompt: promptText,
          },
        } as Partial<VideoElement>);

        // @ts-ignore
        setEdges((eds: any[]) =>
          eds.map((edge: any) =>
            edge.target === videoId
              ? {
                ...edge,
                animated: false,
                style: EDGE_DEFAULT_STYLE,
              }
              : edge
          )
        );
      } catch (error) {
        console.error('âŒ å›¾ç”Ÿè§†é¢‘ç”Ÿæˆå¤±è´¥:', error);
        updateElement(videoId, {
          status: 'error',
          readyForGeneration: true,
        } as Partial<VideoElement>);

        // @ts-ignore
        setEdges((eds: any[]) =>
          eds.map((edge: any) =>
            edge.target === videoId
              ? {
                ...edge,
                animated: false,
                style: { stroke: '#ef4444', strokeWidth: 1 },
              }
              : edge
          )
        );
      } finally {
        activeGenerationRef.current.delete(videoId);
      }
    },
    [setEdges, updateElement]
  );

  useEffect(() => {
    useCanvasStore.setState({
      triggerVideoGeneration: (videoId: string) => {
        void maybeStartVideo(videoId);
      },
    });
    return () => {
      useCanvasStore.setState({ triggerVideoGeneration: undefined });
    };
  }, [maybeStartVideo]);

  // è¡Œçº§æ³¨é‡Šï¼šæ³¨å†Œä»è¾“å…¥æ¡†ç”Ÿæˆå›¾ç‰‡çš„å›è°ƒ
  const handleGenerateFromInput = useCallback(
    async (
      prompt: string,
      aspectRatio: '16:9' | '9:16' | '1:1',
      count: number,
      panelRef: HTMLDivElement | null
    ) => {
      const { elements: storeElements, selection, addPromptHistory } = useCanvasStore.getState();
      const position = getPositionAboveInput(panelRef, reactFlowInstance.screenToFlowPosition);

      // è¡Œçº§æ³¨é‡Šï¼šè·å–é€‰ä¸­çš„å›¾ç‰‡
      const selectedImages = storeElements
        .filter((el) => selection.includes(el.id) && el.type === 'image')
        .map((el) => el as ImageElement);

      try {
        if (selectedImages.length === 0) {
          // è¡Œçº§æ³¨é‡Šï¼šæ–‡ç”Ÿå›¾
          await generateFromInput(
            prompt,
            aspectRatio,
            count,
            position,
            addElement,
            updateElement,
            useCanvasStore.getState().deleteElement,
            addPromptHistory
          );
        } else if (selectedImages.length === 1) {
          // è¡Œçº§æ³¨é‡Šï¼šå›¾ç”Ÿå›¾
          await imageToImageFromInput(
            prompt,
            aspectRatio,
            count,
            selectedImages[0],
            addElement,
            updateElement,
            useCanvasStore.getState().deleteElement,
            addPromptHistory,
            setEdges
          );
        } else {
          // è¡Œçº§æ³¨é‡Šï¼šå¤šå›¾èåˆ
          await multiImageRecipeFromInput(
            prompt,
            aspectRatio,
            count,
            selectedImages,
            addElement,
            updateElement,
            useCanvasStore.getState().deleteElement,
            addPromptHistory,
            setEdges
          );
        }
      } catch (error: any) {
        console.error('ç”Ÿæˆå¤±è´¥:', error);
      }
    },
    [addElement, updateElement, setEdges, reactFlowInstance]
  );

  useEffect(() => {
    useCanvasStore.setState({
      onGenerateFromInput: handleGenerateFromInput,
    });
    return () => {
      useCanvasStore.setState({ onGenerateFromInput: undefined });
    };
  }, [handleGenerateFromInput]);

  const createVideoNodeFromImage = useCallback(
    (
      imageNode: ImageElement,
      flowPosition: { x: number; y: number },
      targetHandleId: 'start-image' | 'end-image' = 'start-image',
      sourceHandleId?: string | null
    ) => {
      const videoId = `video-${Date.now()}`;
      const baseSize = imageNode.size && imageNode.size.width && imageNode.size.height
        ? imageNode.size
        : VIDEO_NODE_DEFAULT_SIZE;

      const nextVideoSize = {
        width: baseSize.width,
        height: baseSize.height,
      };

      const position = {
        x: flowPosition.x - nextVideoSize.width / 2,
        y: flowPosition.y - nextVideoSize.height / 2,
      };

      const startImageInfo =
        targetHandleId === 'start-image'
          ? { startImageId: imageNode.id, startImageUrl: imageNode.src }
          : {};
      const endImageInfo =
        targetHandleId === 'end-image'
          ? { endImageId: imageNode.id, endImageUrl: imageNode.src }
          : {};

      const newVideo: VideoElement = {
        id: videoId,
        type: 'video',
        src: '',
        thumbnail: '',
        duration: 0,
        status: 'pending',
        progress: 0,
        position,
        size: { ...nextVideoSize },
        readyForGeneration: false,
        ...startImageInfo,
        ...endImageInfo,
      };

      addElement(newVideo);

      const edgeId = `edge-${imageNode.id}-${videoId}-${targetHandleId}`;
      // @ts-ignore
      setEdges((eds: any[]) => {
        const filtered = (eds as any[]).filter((edge: any) => edge.id !== edgeId);
        return [
          ...filtered,
          {
            id: edgeId,
            source: imageNode.id,
            target: videoId,
            sourceHandle: sourceHandleId ?? undefined,
            targetHandle: targetHandleId,
            type: 'default',
            animated: false,
            style: EDGE_DEFAULT_STYLE,
          },
        ];
      });
    },
    [addElement, setEdges]
  );

  const createTextNodeForVideo = useCallback(
    (videoNode: VideoElement, flowPosition: { x: number; y: number }) => {
      const textId = `text-${Date.now()}`;
      const textContent = videoNode.promptText || 'åŒå‡»ç¼–è¾‘æ–‡å­—';

      // æ ¹æ®æ–‡å­—å†…å®¹è®¡ç®—èŠ‚ç‚¹å°ºå¯¸
      const fontSize = 24;
      const tempDiv = document.createElement('div');
      tempDiv.style.position = 'absolute';
      tempDiv.style.visibility = 'hidden';
      tempDiv.style.fontSize = `${fontSize}px`;
      tempDiv.style.fontWeight = 'normal';
      tempDiv.style.whiteSpace = 'pre-wrap';
      tempDiv.style.lineHeight = '1.4';
      tempDiv.style.maxWidth = '600px';
      tempDiv.textContent = textContent;

      document.body.appendChild(tempDiv);
      const width = Math.ceil(tempDiv.offsetWidth);
      const height = Math.ceil(tempDiv.offsetHeight);
      document.body.removeChild(tempDiv);

      // æ·»åŠ å†…è¾¹è·
      const padding = 24;
      const calculatedSize = {
        width: Math.max(100, Math.min(600, width + padding * 2)),
        height: Math.max(60, Math.min(400, height + padding * 2)),
      };

      const position = {
        x: flowPosition.x - calculatedSize.width / 2,
        y: flowPosition.y - calculatedSize.height / 2,
      };

      const newText: TextElement = {
        id: textId,
        type: 'text',
        text: textContent,
        position,
        size: calculatedSize,
        fontSize,
      };

      addElement(newText);

      const edgeId = `edge-${textId}-${videoNode.id}-prompt-text`;
      // @ts-ignore
      setEdges((eds: any[]) => {
        const filtered = (eds as any[]).filter((edge: any) => edge.id !== edgeId);
        return [
          ...filtered,
          {
            id: edgeId,
            source: textId,
            target: videoNode.id,
            targetHandle: 'prompt-text',
            type: 'default',
            animated: false,
            style: EDGE_DEFAULT_STYLE,
          },
        ];
      });

      const promptText = videoNode.promptText ?? '';
      const sourceIds = new Set<string>(videoNode.generatedFrom?.sourceIds ?? []);
      if (videoNode.startImageId) {
        sourceIds.add(videoNode.startImageId);
      }
      if (videoNode.endImageId) {
        sourceIds.add(videoNode.endImageId);
      }
      sourceIds.add(textId);

      updateElement(videoNode.id, {
        promptText,
        readyForGeneration: Boolean(promptText.trim() && (videoNode.startImageId || videoNode.endImageId)),
        generatedFrom: {
          type: 'image-to-image',
          sourceIds: Array.from(sourceIds),
          prompt: promptText,
        },
      } as Partial<VideoElement>);
    },
    [addElement, setEdges, updateElement]
  );

  // è¡Œçº§æ³¨é‡Šï¼šå°† store ä¸­çš„å…ƒç´ è½¬æ¢ä¸º React Flow èŠ‚ç‚¹
  // @ts-ignore - React Flow ç±»å‹æ¨æ–­é—®é¢˜
  const nodes: Node[] = useMemo(() => {
    return elements.map((el: CanvasElement) => ({
      id: el.id,
      type: el.type,
      position: el.position,
      data: el,
      draggable: true,
      style: el.size ? {
        width: el.size.width,
        height: el.size.height,
      } : undefined,
    }));
  }, [elements]);

  // è¡Œçº§æ³¨é‡Šï¼šç§»é™¤æ­¤ useEffectï¼Œé¿å…é‡å¤æ¸²æŸ“ï¼ˆå·²ç»åœ¨ä¸Šé¢çš„ useEffect ä¸­å¤„ç†äº†èŠ‚ç‚¹åŒæ­¥ï¼‰

  // è¡Œçº§æ³¨é‡Šï¼šç§»é™¤æŒ‡å‘å·²åˆ é™¤èŠ‚ç‚¹çš„è¿çº¿
  useEffect(() => {
    // @ts-ignore
    setEdges((currentEdges: any[]) =>
      currentEdges.filter((edge: any) => {
        const sourceExists = elements.some((el) => el.id === edge.source);
        const targetExists = elements.some((el) => el.id === edge.target);
        return sourceExists && targetExists;
      })
    );
  }, [elements, setEdges]);

  // æ‹¦æˆª onNodesChangeï¼Œå¤„ç†åˆ é™¤äº‹ä»¶å¹¶åŒæ­¥åˆ° store
  const handleNodesChange = useCallback(
    (changes: any[]) => {
      // è¡Œçº§æ³¨é‡Šï¼šè¿‡æ»¤æ‰æ­£åœ¨ç”Ÿæˆ/å¤„ç†ä¸­çš„èŠ‚ç‚¹åˆ é™¤æ“ä½œ
      const filteredChanges = changes.filter((change) => {
        if (change.type === 'remove') {
          const element = elements.find((el) => el.id === change.id);

          if (element) {
            // è¡Œçº§æ³¨é‡Šï¼šæ£€æŸ¥è§†é¢‘èŠ‚ç‚¹æ˜¯å¦æ­£åœ¨ç”Ÿæˆ
            if (element.type === 'video') {
              const videoElement = element as VideoElement;
              if (videoElement.status === 'queued' || videoElement.status === 'generating') {
                alert('è§†é¢‘æ­£åœ¨ç”Ÿæˆä¸­ï¼Œæ— æ³•åˆ é™¤');
                return false; // é˜»æ­¢åˆ é™¤
              }
            }

            // è¡Œçº§æ³¨é‡Šï¼šæ£€æŸ¥å›¾ç‰‡èŠ‚ç‚¹æ˜¯å¦æ­£åœ¨å¤„ç†
            if (element.type === 'image') {
              const imageElement = element as ImageElement;
              const isSyncing = imageElement.uploadState === 'syncing';
              const hasMediaId = Boolean(imageElement.mediaGenerationId);
              const isError = imageElement.uploadState === 'error';
              const isProcessing = !isError && (isSyncing || !hasMediaId);

              if (isProcessing) {
                alert('å›¾ç‰‡æ­£åœ¨ç”Ÿæˆ/å¤„ç†ä¸­ï¼Œæ— æ³•åˆ é™¤');
                return false; // é˜»æ­¢åˆ é™¤
              }
            }
          }

          // è¡Œçº§æ³¨é‡Šï¼šå…è®¸åˆ é™¤ï¼Œä» store ä¸­åˆ é™¤å…ƒç´ 
          useCanvasStore.getState().deleteElement(change.id);
        }

        return true; // ä¿ç•™è¿™ä¸ªå˜åŒ–
      });

      // è¡Œçº§æ³¨é‡Šï¼šä¼ é€’è¿‡æ»¤åçš„å˜åŒ–ç»™ React Flow
      // æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ onNodesChangeï¼Œå› ä¸º React Flow ä¼šè‡ªåŠ¨å¤„ç† remove
      // ä½†æ˜¯æˆ‘ä»¬éœ€è¦ç¡®ä¿ store åŒæ­¥ã€‚
      // å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œè°ƒç”¨äº† deleteElementï¼Œstore ä¼šæ›´æ–°ï¼Œelements ä¼šå˜ï¼ŒuseEffect ä¼šæ›´æ–° nodesã€‚
      // æ‰€ä»¥å…¶å®æˆ‘ä»¬ä¸éœ€è¦æŠŠ remove å˜åŒ–ä¼ é€’ç»™ onNodesChangeï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´å†²çªï¼Ÿ
      // ä¸ï¼ŒReact Flow çš„ onNodesChange æ˜¯ä¸ºäº†è®©éå—æ§æ¨¡å¼å·¥ä½œï¼Œæˆ–è€…é€šçŸ¥çˆ¶ç»„ä»¶ã€‚
      // åœ¨å—æ§æ¨¡å¼ä¸‹ï¼ˆæˆ‘ä»¬ä½¿ç”¨äº† nodes å±æ€§ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ–° nodesã€‚
      // ä½†æ˜¯æˆ‘ä»¬çš„ nodes æ˜¯ä» elements æ´¾ç”Ÿçš„ã€‚
      // æ‰€ä»¥ï¼Œå½“ deleteElement è¢«è°ƒç”¨ï¼Œelements æ›´æ–°ï¼Œnodes ä¹Ÿä¼šæ›´æ–°ã€‚
      // å¦‚æœæˆ‘ä»¬æŠŠ remove change ä¼ ç»™ onNodesChangeï¼Œå®ƒå¯èƒ½ä¼šè¯•å›¾æ›´æ–°æœ¬åœ° nodes stateã€‚
      // ä½†æˆ‘ä»¬çš„ setNodes æ˜¯åœ¨ useEffect ä¸­è¢« elements è¦†ç›–çš„ã€‚
      // å…³é”®é—®é¢˜æ˜¯ï¼šdeleteElement ä¼šè°ƒç”¨ moveToTrashã€‚
      // å¦‚æœ onNodesChange è¢«è§¦å‘ï¼ˆä¾‹å¦‚æŒ‰ Backspaceï¼‰ï¼Œæˆ‘ä»¬è°ƒç”¨ deleteElementã€‚
      // å¦‚æœæˆ‘ä»¬åŒæ—¶è®© React Flow å¤„ç†è¿™ä¸ª changeï¼Œå®ƒå¯èƒ½ä¼šåœ¨ UI ä¸Šç§»é™¤èŠ‚ç‚¹ã€‚
      // ä½†æœ€ç»ˆ source of truth æ˜¯ elementsã€‚

      // è¿™é‡Œçš„é€»è¾‘çœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ï¼šæ‹¦æˆª removeï¼Œè°ƒç”¨ store deleteï¼Œç„¶åè®© React Flow åšå®ƒçš„äº‹ï¼ˆæˆ–è€…å¿½ç•¥ï¼Œå› ä¸º store ä¼šæ›´æ–°ï¼‰ã€‚
      // ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬å¯ä»¥åªè°ƒç”¨ deleteElementï¼Œä¸ä¼ é€’ remove change ç»™ onNodesChangeï¼Œ
      // å› ä¸º store æ›´æ–°ä¼šè§¦å‘ useEffect æ›´æ–° nodesã€‚
      // ä½†æ˜¯ï¼Œdrag ç­‰å…¶ä»– changes éœ€è¦ä¼ é€’ã€‚

      const nonRemoveChanges = changes.filter(c => c.type !== 'remove');
      if (nonRemoveChanges.length > 0) {
        onNodesChange(nonRemoveChanges);
      }
    },
    [onNodesChange, elements]
  );

  // è¡Œçº§æ³¨é‡Šï¼šæ‹–åŠ¨è¿‡ç¨‹ä¸­çš„èŠ‚ç‚¹ä½ç½®ç¼“å­˜ï¼ˆé¿å…é¢‘ç¹æ›´æ–° storeï¼‰
  const draggedNodesRef = useRef<Map<string, { x: number; y: number }>>(new Map());

  // è¡Œçº§æ³¨é‡Šï¼šæ‹–åŠ¨è¿‡ç¨‹ä¸­åªæ›´æ–°æœ¬åœ°çŠ¶æ€ï¼Œä¸è§¦å‘ store æ›´æ–°ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
  const handleNodeDrag = useCallback(
    (_event: React.MouseEvent, node: Node) => {
      draggedNodesRef.current.set(node.id, { x: node.position.x, y: node.position.y });
    },
    []
  );

  // è¡Œçº§æ³¨é‡Šï¼šæ‹–åŠ¨ç»“æŸåæ‰¹é‡æ›´æ–° storeï¼ˆå‡å°‘æ¸²æŸ“æ¬¡æ•°ï¼‰
  const handleNodeDragStop = useCallback(
    (_event: React.MouseEvent, node: Node, nodes: Node[]) => {
      // æ‰¾åˆ°æ‰€æœ‰è¢«æ‹–åŠ¨çš„èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬å¤šé€‰ï¼‰
      const selectedNodes = nodes.filter(n => n.selected);
      const nodesToUpdate = selectedNodes.length > 1 ? selectedNodes : [node];

      // è¡Œçº§æ³¨é‡Šï¼šæ‰¹é‡æ›´æ–°ä½ç½®åˆ° storeï¼ˆä¸€æ¬¡æ€§æ›´æ–°ï¼Œé¿å…å¤šæ¬¡è§¦å‘ elements å˜åŒ–ï¼‰
      const { elements: currentElements } = useCanvasStore.getState();
      const updatedElements = currentElements.map((el) => {
        const draggedNode = nodesToUpdate.find((n) => n.id === el.id);
        if (draggedNode) {
          return { ...el, position: draggedNode.position };
        }
        return el;
      });

      // è¡Œçº§æ³¨é‡Šï¼šç›´æ¥æ›¿æ¢æ•´ä¸ª elements æ•°ç»„ï¼ˆä¸€æ¬¡æ€§æ›´æ–°ï¼Œè€Œéå¤šæ¬¡è°ƒç”¨ updateElementï¼‰
      useCanvasStore.setState({ elements: updatedElements });

      // æ¸…ç©ºæ‹–åŠ¨ç¼“å­˜
      draggedNodesRef.current.clear();
    },
    []
  );

  // å¤„ç†é€‰ä¸­å˜åŒ–
  const handleSelectionChange = useCallback(
    (params: OnSelectionChangeParams) => {
      const selectedIds = params.nodes.map((node) => node.id);
      setSelection(selectedIds);
    },
    [setSelection]
  );

  // å¤„ç†è¿çº¿å¼€å§‹ - è®°å½•æºèŠ‚ç‚¹ä¿¡æ¯
  const handleConnectStart = useCallback<OnConnectStart>(
    (_event, params) => {
      const sourceNode = elements.find((el) => el.id === params.nodeId);

      if (!sourceNode) {
        connectionStartRef.current = null;
        resetConnectionMenu();
        return;
      }

      connectionStartRef.current = {
        sourceId: sourceNode.id,
        sourceType: sourceNode.type,
        handleId: params.handleId,
        didConnect: false,
      };

      prepareConnectionMenu(sourceNode.id, sourceNode.type as CanvasElement['type']);
    },
    [elements, prepareConnectionMenu, resetConnectionMenu]
  );

  // å¤„ç†è¿çº¿ç»“æŸ - æ˜¾ç¤ºé€‰é¡¹èœå•
  const handleConnectEnd = useCallback<OnConnectEnd>(
    (event) => {
      const startInfo = connectionStartRef.current;
      connectionStartRef.current = null;

      const mouseEvent = event as MouseEvent;
      const targetElement = mouseEvent?.target as HTMLElement | null;
      const droppedOnPane = targetElement?.classList?.contains('react-flow__pane');

      if (!startInfo) {
        resetConnectionMenu();
        return;
      }

      if (startInfo.didConnect) {
        resetConnectionMenu();
        return;
      }

      if (!droppedOnPane) {
        resetConnectionMenu();
        return;
      }

      if (startInfo.sourceType === 'text') {
        const sourceNode = elements.find((el) => el.id === startInfo.sourceId);
        if (!sourceNode || sourceNode.type !== 'text') {
          resetConnectionMenu();
          return;
        }

        showConnectionMenu(
          { x: mouseEvent.clientX, y: mouseEvent.clientY },
          sourceNode.id,
          'text'
        );
        return;
      }

      if (startInfo.sourceType === 'video') {
        const videoNode = elements.find((el) => el.id === startInfo.sourceId) as VideoElement | undefined;
        if (!videoNode) {
          resetConnectionMenu();
          return;
        }

        if (startInfo.handleId === 'prompt-text') {
          const flowPosition = reactFlowInstance.screenToFlowPosition({
            x: mouseEvent.clientX,
            y: mouseEvent.clientY,
          });

          createTextNodeForVideo(videoNode, flowPosition);
          resetConnectionMenu();
          return;
        }

        // è¡Œçº§æ³¨é‡Šï¼šè§†é¢‘èŠ‚ç‚¹æ‹‰å‡ºè¿çº¿ï¼ˆé prompt-text handleï¼‰ï¼Œæ˜¾ç¤ºé•œå¤´æ§åˆ¶èœå•
        showConnectionMenu(
          { x: mouseEvent.clientX, y: mouseEvent.clientY },
          videoNode.id,
          'video'
        );
        return;
      }

      if (startInfo.sourceType === 'image') {
        const sourceNode = elements.find((el) => el.id === startInfo.sourceId) as ImageElement | undefined;
        if (!sourceNode) {
          resetConnectionMenu();
          return;
        }

        // è¡Œçº§æ³¨é‡Šï¼šå›¾ç‰‡èŠ‚ç‚¹æ‹‰çº¿æ—¶ä¹Ÿæ˜¾ç¤ºèœå•ï¼Œè®©ç”¨æˆ·é€‰æ‹©ç”Ÿæˆå›¾ç‰‡è¿˜æ˜¯è§†é¢‘
        showConnectionMenu(
          { x: mouseEvent.clientX, y: mouseEvent.clientY },
          sourceNode.id,
          'image'
        );
        return;
      }
    },
    [elements, createTextNodeForVideo, reactFlowInstance, resetConnectionMenu, showConnectionMenu]
  );

  // å¤„ç†é€‰æ‹©ç”Ÿæˆå›¾ç‰‡ï¼ˆå¸¦æ¯”ä¾‹å‚æ•°ï¼‰
  const handleGenerateImage = useCallback(
    async (aspectRatio: ImageAspectRatio) => {
      const sourceNodeId = connectionMenu.sourceNodeId;
      const sourceNodeType = connectionMenu.sourceNodeType;
      if (!sourceNodeId || !sourceNodeType) return;

      const sourceNode = elements.find((el) => el.id === sourceNodeId);
      if (!sourceNode) return;

      // è¡Œçº§æ³¨é‡Šï¼šä»æ–‡å­—èŠ‚ç‚¹ç”Ÿæˆå›¾ç‰‡ï¼ˆæ–‡ç”Ÿå›¾ï¼‰
      if (sourceNodeType === 'text' && sourceNode.type === 'text') {
        handleTextToImage(sourceNode as TextElement, aspectRatio);
        return;
      }

      // è¡Œçº§æ³¨é‡Šï¼šä»å›¾ç‰‡èŠ‚ç‚¹ç”Ÿæˆå›¾ç‰‡ï¼ˆå›¾ç”Ÿå›¾ï¼‰
      if (sourceNodeType === 'image' && sourceNode.type === 'image') {
        showImagePromptInput(aspectRatio);
        return;
      }
    },
    [connectionMenu.sourceNodeId, connectionMenu.sourceNodeType, elements, handleTextToImage, showImagePromptInput]
  );

  // è¡Œçº§æ³¨é‡Šï¼šå›¾ç”Ÿå›¾æç¤ºè¯è¾“å…¥å˜åŒ–å¤„ç†ï¼ˆç°åœ¨ç”± Hook ç®¡ç†ï¼‰
  const handleImagePromptInputChange = useCallback(
    (value: string) => {
      updateImagePrompt(value);
    },
    [updateImagePrompt]
  );

  const handleConfirmImagePrompt = useCallback(() => {
    const config = connectionMenu.pendingImageConfig;
    const sourceNodeId = connectionMenu.sourceNodeId;
    if (!config || !sourceNodeId) {
      return;
    }

    const promptText = config.prompt.trim();
    if (!promptText) {
      alert('è¯·è¾“å…¥æç¤ºè¯');
      return;
    }

    const sourceNode = elements.find(
      (el) => el.id === sourceNodeId && el.type === 'image'
    ) as ImageElement | undefined;

    if (!sourceNode) {
      resetConnectionMenu();
      return;
    }

    handleImageToImage(sourceNode, config.aspectRatio, promptText);
  }, [
    connectionMenu.pendingImageConfig,
    connectionMenu.sourceNodeId,
    elements,
    handleImageToImage,
    resetConnectionMenu,
  ]);

  // å¤„ç†é€‰æ‹©ç”Ÿæˆè§†é¢‘
  const handleGenerateVideo = useCallback(
    async (aspectRatio: '9:16' | '16:9') => {
      const sourceNodeId = connectionMenu.sourceNodeId;
      const sourceNodeType = connectionMenu.sourceNodeType;
      if (!sourceNodeId || !sourceNodeType) return;

      const sourceNode = elements.find((el) => el.id === sourceNodeId);
      if (!sourceNode) return;

      // è¡Œçº§æ³¨é‡Šï¼šä»æ–‡å­—èŠ‚ç‚¹ç”Ÿæˆè§†é¢‘
      if (sourceNodeType === 'text' && sourceNode.type === 'text') {
        handleTextToVideo(sourceNode as TextElement, aspectRatio);
        return;
      }

      // è¡Œçº§æ³¨é‡Šï¼šä»å›¾ç‰‡èŠ‚ç‚¹ç”Ÿæˆè§†é¢‘
      if (sourceNodeType === 'image' && sourceNode.type === 'image') {
        handleImageToVideo(sourceNode as ImageElement, aspectRatio);
        return;
      }
    },
    [connectionMenu.sourceNodeId, connectionMenu.sourceNodeType, elements]
  );

  // å›¾ç‰‡ç¼–è¾‘å™¨å›è°ƒå‡½æ•° - ä½¿ç”¨ useCallback é¿å…ä¸å¿…è¦çš„é‡æ–°æ¸²æŸ“
  const handleAnnotatorClose = useCallback(() => {
    setAnnotatorTarget(null);
    setMainImageForEdit(null); // æ¸…ç©ºä¸»å›¾
    setReferenceImages([]); // æ¸…ç©ºå‚è€ƒå›¾
  }, [setAnnotatorTarget]);

  // è¡Œçº§æ³¨é‡Šï¼šå¤šå›¾ç¼–è¾‘ - ç”¨æˆ·ä»ç”»å¸ƒé€‰ä¸­å¤šå¼ å›¾ç‰‡åç‚¹å‡»"å›¾ç‰‡ç¼–è¾‘"
  const handleMultiImageEdit = useCallback(async () => {
    const selection = useCanvasStore.getState().selection;
    const selectedImages = elements
      .filter((el) => selection.includes(el.id) && el.type === 'image')
      .map((el) => el as ImageElement);

    if (selectedImages.length < 2 || selectedImages.length > 6) {
      console.error('å¤šå›¾ç¼–è¾‘éœ€è¦ 2-6 å¼ å›¾ç‰‡');
      return;
    }

    // ç¬¬1å¼ ä½œä¸ºä¸»å›¾ï¼Œå…¶ä»–ä½œä¸ºå‚è€ƒå›¾
    const mainImage = selectedImages[0];
    const refImages = selectedImages.slice(1);

    // åŠ è½½ä¸»å›¾
    setIsLoadingAnnotatorImage(true);

    try {
      const apiConfig = useCanvasStore.getState().apiConfig;

      // è¡Œçº§æ³¨é‡Šï¼šå°† API é…ç½®æš´éœ²åˆ° windowï¼Œä¾› ImageAnnotatorModal ä½¿ç”¨
      if (typeof window !== 'undefined') {
        (window as any).__API_KEY__ = apiConfig.apiKey || '';
        (window as any).__PROXY__ = apiConfig.proxy || '';
        (window as any).__BEARER_TOKEN__ = apiConfig.bearerToken || '';
      }

      // è¡Œçº§æ³¨é‡Šï¼šåŠ è½½ä¸»å›¾çš„ base64 æ•°æ®
      let mainImageBase64Src: string;

      // å¦‚æœä¸»å›¾æœ‰ base64ï¼Œç›´æ¥ä½¿ç”¨
      if (mainImage.base64) {
        mainImageBase64Src = mainImage.base64.startsWith('data:')
          ? mainImage.base64
          : `data:image/png;base64,${mainImage.base64}`;
      } else {
        // å¦‚æœæ²¡æœ‰ base64ï¼Œé€šè¿‡ API è·å–
        const effectiveMediaId = mainImage.mediaId || mainImage.mediaGenerationId;

        if (!effectiveMediaId) {
          throw new Error('ä¸»å›¾ç¼ºå°‘ mediaIdï¼Œæ— æ³•ç¼–è¾‘');
        }

        if (!apiConfig.bearerToken) {
          throw new Error('è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® Bearer Token');
        }

        const mediaResponse = await fetch(
          `/api/flow/media/${effectiveMediaId}?key=${apiConfig.apiKey}&returnUriOnly=false&proxy=${apiConfig.proxy || ''}`,
          {
            headers: apiConfig.bearerToken ? {
              'Authorization': `Bearer ${apiConfig.bearerToken}`
            } : {}
          }
        );

        if (!mediaResponse.ok) {
          throw new Error('è·å–å›¾ç‰‡å¤±è´¥');
        }

        const mediaData = await mediaResponse.json();
        const encodedImage = mediaData?.image?.encodedImage ||
          mediaData?.userUploadedImage?.image ||
          mediaData?.userUploadedImage?.encodedImage;

        if (!encodedImage) {
          throw new Error('æœªè·å–åˆ°å›¾ç‰‡æ•°æ®');
        }

        mainImageBase64Src = encodedImage.startsWith('data:')
          ? encodedImage
          : `data:image/png;base64,${encodedImage}`;
      }

      // è¡Œçº§æ³¨é‡Šï¼šç¡®ä¿å‚è€ƒå›¾ä¹ŸåŒ…å« base64ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œç”¨äºåˆ‡æ¢ä¸»å›¾
      const refImagesWithBase64 = refImages.map(img => {
        if (img.base64) {
          const base64Src = img.base64.startsWith('data:')
            ? img.base64
            : `data:image/png;base64,${img.base64}`;
          return { ...img, base64: base64Src };
        }
        return img;
      });

      setAnnotatorTarget({
        ...mainImage,
        src: mainImageBase64Src,
        base64: mainImageBase64Src,
      });
      setMainImageForEdit({
        ...mainImage,
        base64: mainImageBase64Src,
      });
      setReferenceImages(refImagesWithBase64);
      setIsLoadingAnnotatorImage(false);

    } catch (error) {
      console.error('âŒ åŠ è½½ä¸»å›¾å¤±è´¥:', error);
      setAnnotatorTarget(null);
      setMainImageForEdit(null);
      setReferenceImages([]);
      setIsLoadingAnnotatorImage(false);
    }
  }, [elements, setAnnotatorTarget, setIsLoadingAnnotatorImage]);

  const handleAnnotatorConfirm = useCallback(async (
    result: ImageAnnotatorResult,
    annotatedImageDataUrl: string,
    finalMainImage?: ImageElement,
    finalReferenceImages?: ImageElement[]
  ) => {
    // è¡Œçº§æ³¨é‡Šï¼šä½¿ç”¨ç”¨æˆ·æœ€ç»ˆç¡®è®¤çš„ä¸»å›¾å’Œå‚è€ƒå›¾ï¼ˆå¯èƒ½è¢«åˆ‡æ¢è¿‡ï¼‰
    const currentMainImage = finalMainImage || annotatorTarget;
    const currentReferenceImages = finalReferenceImages || referenceImages;

    if (!currentMainImage || !result.promptText?.trim()) return;

    const newImageId = `image-${Date.now()}`;
    const hasReferenceImages = currentReferenceImages.length > 0;
    const allSourceImages = [currentMainImage, ...currentReferenceImages];

    try {
      const aspectRatio = (() => {
        const width = currentMainImage.size?.width || 640;
        const height = currentMainImage.size?.height || 360;
        const ratio = width / height;
        if (Math.abs(ratio - 16 / 9) < 0.1) return '16:9';
        if (Math.abs(ratio - 9 / 16) < 0.1) return '9:16';
        return '1:1';
      })() as '16:9' | '9:16' | '1:1';

      const size = getImageNodeSize(aspectRatio);

      const newImage: ImageElement = {
        id: newImageId,
        type: 'image',
        src: '',
        position: {
          x: currentMainImage.position.x + (currentMainImage.size?.width || 640) + 50,
          y: currentMainImage.position.y,
        },
        size,
        sourceImageIds: allSourceImages.map(img => img.id),
        generatedFrom: {
          type: 'image-to-image',
          sourceIds: allSourceImages.map(img => img.id),
          prompt: result.promptText,
        },
      };

      addElement(newImage);

      // è¡Œçº§æ³¨é‡Šï¼šä¸ºæ‰€æœ‰æºå›¾ç‰‡åˆ›å»ºè¿çº¿ï¼ˆä¸»å›¾ + å‚è€ƒå›¾ï¼‰
      const edgeIds: string[] = [];
      // @ts-ignore
      setEdges((eds: any[]) => {
        const newEdges = allSourceImages.map(sourceImg => {
          const edgeId = `edge-${sourceImg.id}-${newImageId}-edit`;
          edgeIds.push(edgeId);
          return {
            id: edgeId,
            source: sourceImg.id,
            sourceHandle: null,
            target: newImageId,
            targetHandle: null,
            type: 'default',
            animated: true,
            style: { stroke: '#a855f7', strokeWidth: 1 },
          };
        });
        return [...eds, ...newEdges];
      });

      // ä¸Šä¼ æ ‡æ³¨å›¾
      const base64Data = annotatedImageDataUrl.split(',')[1];
      const uploadResult = await registerUploadedImage(base64Data);
      if (!uploadResult.mediaGenerationId) throw new Error('ä¸Šä¼ æ ‡æ³¨å›¾å¤±è´¥');

      let imageResult;

      if (hasReferenceImages) {
        // è¡Œçº§æ³¨é‡Šï¼šå¤šå›¾ç¼–è¾‘ - ä½¿ç”¨ runImageRecipeï¼ˆä¸ä½¿ç”¨å‰ç½®æç¤ºè¯ï¼‰
        console.log('ğŸ§© å¤šå›¾èåˆæ¨¡å¼ï¼Œå‚è€ƒå›¾æ•°é‡:', referenceImages.length);

        const { runImageRecipe } = await import('@/lib/api-mock');

        // æ„å»ºå‚è€ƒå›¾åˆ—è¡¨
        const references = [
          // ä¸»å›¾ï¼ˆæ ‡æ³¨åï¼‰
          {
            mediaId: uploadResult.mediaGenerationId,
            caption: 'æ ‡æ³¨åçš„ä¸»å›¾',
            mediaCategory: 'MEDIA_CATEGORY_BOARD',
          },
          // å‚è€ƒå›¾
          ...currentReferenceImages.map((ref, index) => ({
            mediaId: ref.mediaId || ref.mediaGenerationId,
            caption: ref.caption || `å‚è€ƒå›¾${index + 1}`,
            mediaCategory: 'MEDIA_CATEGORY_SUBJECT',
          }))
        ];

        // æ£€æŸ¥æ‰€æœ‰å›¾ç‰‡æ˜¯å¦æœ‰ mediaId
        for (const ref of references) {
          if (!ref.mediaId) {
            throw new Error('å­˜åœ¨æœªåŒæ­¥åˆ° Flow çš„å‚è€ƒå›¾ï¼Œè¯·ç¨åé‡è¯•');
          }
        }

        imageResult = await runImageRecipe(
          result.promptText,
          references,
          aspectRatio,
          undefined,
          1
        );

      } else {
        // è¡Œçº§æ³¨é‡Šï¼šå•å›¾ç¼–è¾‘ - ä½¿ç”¨ imageToImageï¼ˆä¸ä½¿ç”¨å‰ç½®æç¤ºè¯ï¼‰
        console.log('ğŸ¨ å•å›¾ç¼–è¾‘æ¨¡å¼');

        imageResult = await imageToImage(
          result.promptText,
          annotatedImageDataUrl,
          aspectRatio,
          '',
          uploadResult.mediaGenerationId,
          1
        );
      }

      // æ›´æ–°å›¾ç‰‡
      updateElement(newImageId, {
        src: imageResult.imageUrl,
        base64: imageResult.images?.[0]?.base64,
        promptId: imageResult.promptId,
        mediaId: imageResult.mediaId,
        mediaGenerationId: imageResult.mediaGenerationId,
        uploadState: 'synced',
      } as Partial<ImageElement>);

      // ç”ŸæˆæˆåŠŸåï¼Œåœæ­¢æ‰€æœ‰è¿çº¿åŠ¨ç”»
      // @ts-ignore
      setEdges((eds: any[]) =>
        eds.map((edge: any) =>
          edgeIds.includes(edge.id)
            ? { ...edge, animated: false, style: { stroke: '#10b981', strokeWidth: 1 } }
            : edge
        )
      );

      console.log('âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼');

    } catch (error) {
      console.error('âŒ å›¾ç‰‡ç¼–è¾‘å¤±è´¥:', error);

      // å¦‚æœå¤±è´¥ï¼Œå°†æ‰€æœ‰è¿çº¿æ ‡è®°ä¸ºé”™è¯¯çŠ¶æ€
      // @ts-ignore
      setEdges((eds: any[]) =>
        eds.map((edge: any) =>
          edge.target === newImageId
            ? { ...edge, animated: false, style: { stroke: '#ef4444', strokeWidth: 1 } }
            : edge
        )
      );
    }
  }, [annotatorTarget, addElement, updateElement, setEdges]);

  // è¡Œçº§æ³¨é‡Šï¼šæ–‡ç”Ÿè§†é¢‘å¤„ç†å‡½æ•°
  const handleTextToVideo = useCallback(
    async (sourceNode: TextElement, aspectRatio: '9:16' | '16:9') => {
      resetConnectionMenu();

      const videoSize = getVideoNodeSize(aspectRatio);

      const newVideoId = `video-${Date.now()}`;
      const newVideo: VideoElement = {
        id: newVideoId,
        type: 'video',
        src: '',
        thumbnail: '',
        duration: 0,
        status: 'generating',
        progress: 0,
        position: {
          x: sourceNode.position.x + (sourceNode.size?.width || TEXT_NODE_DEFAULT_SIZE.width) + 100,
          y: sourceNode.position.y,
        },
        size: videoSize,
        promptText: sourceNode.text,
        generatedFrom: {
          type: 'text',
          sourceIds: [sourceNode.id],
          prompt: sourceNode.text,
        },
      };

      addElement(newVideo);

      // åˆ›å»ºè¿çº¿
      // @ts-ignore
      setEdges((eds: any[]) => [
        ...eds,
        {
          id: `edge-${sourceNode.id}-${newVideoId}-prompt-text`,
          source: sourceNode.id,
          target: newVideoId,
          targetHandle: 'prompt-text',
          type: 'default',
          animated: true,
          style: { stroke: '#a855f7', strokeWidth: 1 },
        },
      ]);

      try {
        const result = await generateVideoFromText(sourceNode.text, aspectRatio);

        updateElement(newVideoId, {
          status: 'ready',
          src: result.videoUrl,
          thumbnail: result.thumbnail,
          duration: result.duration,
          progress: 100,
          mediaGenerationId: result.mediaGenerationId,
          promptText: sourceNode.text,
          generatedFrom: {
            type: 'text',
            sourceIds: [sourceNode.id],
            prompt: sourceNode.text,
          },
        } as Partial<VideoElement>);

        // @ts-ignore
        setEdges((eds: any[]) =>
          eds.map((edge: any) =>
            edge.id === `edge-${sourceNode.id}-${newVideoId}-prompt-text`
              ? { ...edge, animated: false }
              : edge
          )
        );

        console.log('âœ… ä»æ–‡æœ¬èŠ‚ç‚¹ç”Ÿæˆè§†é¢‘:', sourceNode.text);
      } catch (error) {
        console.error('âŒ ç”Ÿæˆè§†é¢‘å¤±è´¥:', error);
        // @ts-ignore
        setEdges((eds: any[]) =>
          eds.map((edge: any) =>
            edge.id === `edge-${sourceNode.id}-${newVideoId}-prompt-text`
              ? { ...edge, animated: false, style: { stroke: '#ef4444', strokeWidth: 1 } }
              : edge
          )
        );
        alert('ç”Ÿæˆè§†é¢‘å¤±è´¥ï¼Œè¯·é‡è¯•');
      }
    },
    [addElement, updateElement, setEdges, resetConnectionMenu]
  );

  // è¡Œçº§æ³¨é‡Šï¼šå›¾ç”Ÿè§†é¢‘å¤„ç†å‡½æ•°
  const handleImageToVideo = useCallback(
    async (sourceNode: ImageElement, aspectRatio: '9:16' | '16:9') => {
      resetConnectionMenu();

      const flowPosition = {
        x: sourceNode.position.x + (sourceNode.size?.width || IMAGE_NODE_DEFAULT_SIZE.width) + 100,
        y: sourceNode.position.y,
      };

      // è¡Œçº§æ³¨é‡Šï¼šç›´æ¥è°ƒç”¨ç°æœ‰çš„ createVideoNodeFromImage å‡½æ•°
      createVideoNodeFromImage(sourceNode, flowPosition, 'start-image', 'right');

      console.log('âœ… ä»å›¾ç‰‡èŠ‚ç‚¹åˆ›å»ºè§†é¢‘èŠ‚ç‚¹:', sourceNode.id);
    },
    [createVideoNodeFromImage, resetConnectionMenu]
  );

  // è¡Œçº§æ³¨é‡Šï¼šä»å›¾ç‰‡ç”Ÿæˆè§†é¢‘ï¼ˆè‡ªåŠ¨æ ¹æ®å›¾ç‰‡æ¯”ä¾‹ï¼‰
  const handleGenerateVideoFromImage = useCallback(() => {
    const sourceNodeId = connectionMenu.sourceNodeId;
    if (!sourceNodeId) return;

    const sourceNode = elements.find(
      (el) => el.id === sourceNodeId && el.type === 'image'
    ) as ImageElement | undefined;

    if (!sourceNode) {
      resetConnectionMenu();
      return;
    }

    // è¡Œçº§æ³¨é‡Šï¼šæ ¹æ®å›¾ç‰‡å°ºå¯¸åˆ¤æ–­æ¯”ä¾‹ï¼ˆä¸ ImageNode çš„ getAspectRatio é€»è¾‘ä¸€è‡´ï¼‰
    const width = sourceNode.size?.width || 320;
    const height = sourceNode.size?.height || 180;
    const ratio = width / height;

    // è¡Œçº§æ³¨é‡Šï¼šè§†é¢‘åªæ”¯æŒ 16:9 å’Œ 9:16ï¼Œæ–¹å½¢å›¾ç‰‡é»˜è®¤ç”¨æ¨ªå±
    const aspectRatio: '9:16' | '16:9' = Math.abs(ratio - 9 / 16) < 0.1 ? '9:16' : '16:9';

    console.log('ğŸ¬ æ ¹æ®å›¾ç‰‡æ¯”ä¾‹è‡ªåŠ¨ç”Ÿæˆè§†é¢‘:', { width, height, aspectRatio });

    handleImageToVideo(sourceNode, aspectRatio);
  }, [connectionMenu.sourceNodeId, elements, handleImageToVideo, resetConnectionMenu]);

  // å¤„ç†é•œå¤´æ§åˆ¶é‡æ‹ï¼ˆç”Ÿæˆè§†é¢‘ï¼‰
  const handleGenerateReshoot = useCallback(
    async (motionType: ReshootMotionType) => {
      const sourceNodeId = connectionMenu.sourceNodeId;
      if (!sourceNodeId) return;

      const sourceNode = elements.find((el) => el.id === sourceNodeId) as VideoElement | undefined;
      if (!sourceNode) return;

      resetConnectionMenu();

      // 1. åˆ›å»ºæ–°çš„è§†é¢‘èŠ‚ç‚¹
      const newVideoId = `video-${Date.now()}`;
      const flowPosition = reactFlowInstance.screenToFlowPosition({
        x: connectionMenu.position.x,
        y: connectionMenu.position.y,
      });

      const newVideo: VideoElement = {
        id: newVideoId,
        type: 'video',
        src: '',
        thumbnail: '',
        duration: 0,
        status: 'generating', // ç›´æ¥å¼€å§‹ç”Ÿæˆ
        progress: 0,
        position: { x: flowPosition.x, y: flowPosition.y },
        size: sourceNode.size || VIDEO_NODE_DEFAULT_SIZE,
        generatedFrom: {
          type: 'reshoot',
          sourceIds: [sourceNode.id],
        },
      };

      addElement(newVideo);

      // 2. åˆ›å»ºè¿çº¿
      const edgeId = `edge-${sourceNode.id}-${newVideoId}-reshoot`;
      // @ts-ignore
      setEdges((eds: any[]) => [
        ...eds,
        {
          id: edgeId,
          source: sourceNode.id,
          target: newVideoId,
          type: 'default',
          animated: true,
          style: { stroke: '#a855f7', strokeWidth: 1 },
          label: 'é•œå¤´æ§åˆ¶',
        },
      ]);

      // 3. è°ƒç”¨ API ç”Ÿæˆ
      try {
        const effectiveMediaId = sourceNode.mediaGenerationId;

        if (!effectiveMediaId) {
          throw new Error('æºè§†é¢‘ç¼ºå°‘ mediaGenerationId');
        }

        const aspectRatio = sourceNode.size?.width && sourceNode.size?.height
          ? (Math.abs(sourceNode.size.width / sourceNode.size.height - 16 / 9) < 0.1 ? '16:9' : '9:16')
          : '16:9';

        const { generateVideoReshoot } = await import('@/lib/api-mock');
        const result = await generateVideoReshoot(
          effectiveMediaId,
          motionType,
          aspectRatio as any
        );

        updateElement(newVideoId, {
          status: 'ready',
          src: result.videoUrl,
          thumbnail: result.thumbnail,
          duration: result.duration,
          mediaGenerationId: result.mediaGenerationId,
          progress: 100,
          readyForGeneration: true,
        } as Partial<VideoElement>);

        // @ts-ignore
        setEdges((eds: any[]) =>
          eds.map((edge: any) =>
            edge.id === edgeId
              ? { ...edge, animated: false }
              : edge
          )
        );

        console.log('âœ… é•œå¤´æ§åˆ¶è§†é¢‘ç”ŸæˆæˆåŠŸ');
      } catch (error) {
        console.error('âŒ é•œå¤´æ§åˆ¶è§†é¢‘ç”Ÿæˆå¤±è´¥:', error);
        updateElement(newVideoId, { status: 'error' } as Partial<VideoElement>);
        // @ts-ignore
        setEdges((eds: any[]) =>
          eds.map((edge: any) =>
            edge.id === edgeId
              ? { ...edge, animated: false, style: { stroke: '#ef4444', strokeWidth: 1 } }
              : edge
          )
        );
      }
    },
    [connectionMenu.sourceNodeId, connectionMenu.position, elements, addElement, setEdges, updateElement, reactFlowInstance, resetConnectionMenu]
  );

  // å¤„ç†å»¶é•¿è§†é¢‘ - åˆ›å»º pending èŠ‚ç‚¹
  const handleShowExtendVideo = useCallback(() => {
    const sourceNodeId = connectionMenu.sourceNodeId;
    if (!sourceNodeId) return;

    const sourceNode = elements.find((el) => el.id === sourceNodeId) as VideoElement | undefined;
    if (!sourceNode) return;

    resetConnectionMenu();

    // 1. åˆ›å»º pending çŠ¶æ€çš„è§†é¢‘èŠ‚ç‚¹ï¼ˆç”¨æˆ·ç¨åè¾“å…¥æç¤ºè¯ï¼‰
    const newVideoId = `video-${Date.now()}`;
    const flowPosition = reactFlowInstance.screenToFlowPosition({
      x: connectionMenu.position.x,
      y: connectionMenu.position.y,
    });

    const newVideo: VideoElement = {
      id: newVideoId,
      type: 'video',
      src: '',
      thumbnail: '',
      duration: 0,
      status: 'pending', // è¡Œçº§æ³¨é‡Šï¼špending çŠ¶æ€ä¼šè§¦å‘ VideoNode æ˜¾ç¤ºè¾“å…¥é¢æ¿
      progress: 0,
      position: { x: flowPosition.x, y: flowPosition.y },
      size: sourceNode.size || VIDEO_NODE_DEFAULT_SIZE,
      readyForGeneration: false,
      generatedFrom: {
        type: 'extend',
        sourceIds: [sourceNode.id],
      },
    };

    addElement(newVideo);

    // 2. åˆ›å»ºè¿çº¿
    const edgeId = `edge-${sourceNode.id}-${newVideoId}-extend`;
    // @ts-ignore
    setEdges((eds: any[]) => [
      ...eds,
      {
        id: edgeId,
        source: sourceNode.id,
        target: newVideoId,
        type: 'default',
        animated: false,
        style: { stroke: '#a855f7', strokeWidth: 1 },
        label: 'å»¶é•¿',
      },
    ]);

    console.log('âœ… å»¶é•¿è§†é¢‘èŠ‚ç‚¹å·²åˆ›å»ºï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥æç¤ºè¯');
  }, [connectionMenu.sourceNodeId, connectionMenu.position, elements, addElement, setEdges, reactFlowInstance, resetConnectionMenu]);

  // å¤„ç†è¿çº¿è¿æ¥ï¼ˆç”Ÿæˆè§†é¢‘ï¼‰
  const handleConnect = useCallback(
    (connection: Connection) => {
      if (connectionStartRef.current) {
        connectionStartRef.current.didConnect = true;
      }

      const sourceId = connection.source;
      const targetId = connection.target;

      if (!sourceId || !targetId) {
        return;
      }

      const sourceNode = elements.find((el) => el.id === sourceId);
      const targetNode = elements.find((el) => el.id === targetId);

      if (!sourceNode || !targetNode) {
        return;
      }

      const edgeId = `edge-${sourceId}-${targetId}-${connection.targetHandle || 'default'}`;
      const upsertEdge = (animated = false, style = EDGE_DEFAULT_STYLE) => {
        // @ts-ignore
        setEdges((eds: any[]) => {
          const filtered = (eds as any[]).filter((edge: any) => edge.id !== edgeId);
          return [
            ...filtered,
            {
              id: edgeId,
              source: sourceId,
              target: targetId,
              sourceHandle: connection.sourceHandle,
              targetHandle: connection.targetHandle,
              type: (connection as any).type || 'default',
              animated,
              style,
            },
          ];
        });
      };

      // åªå…è®¸è¿æ¥åˆ°è§†é¢‘èŠ‚ç‚¹çš„è‡ªå®šä¹‰è¾“å…¥ï¼ˆåªæ”¯æŒå›¾ç‰‡è¿æ¥ï¼Œä¸æ”¯æŒæ–‡æœ¬è¿æ¥ï¼‰
      if (targetNode.type === 'video') {
        const targetHandle = connection.targetHandle;
        let handled = false;

        if (sourceNode.type === 'image' && targetHandle === 'start-image') {
          const imageNode = sourceNode as ImageElement;
          const videoData = targetNode as VideoElement;
          const sourceIds = new Set<string>(videoData.generatedFrom?.sourceIds ?? []);
          sourceIds.add(imageNode.id);
          if (videoData.endImageId) {
            sourceIds.add(videoData.endImageId);
          }
          const updates: Partial<VideoElement> = {
            startImageId: imageNode.id,
            startImageUrl: imageNode.src,
            generatedFrom: {
              type: 'image-to-image',
              sourceIds: Array.from(sourceIds),
              prompt: videoData.promptText,
            },
          };
          if (videoData.status === 'ready') {
            updates.status = 'pending';
            updates.progress = 0;
            updates.src = '';
            updates.thumbnail = '';
          }
          updateElement(targetId, updates);
          handled = true;
        } else if (sourceNode.type === 'image' && targetHandle === 'end-image') {
          const imageNode = sourceNode as ImageElement;
          const videoData = targetNode as VideoElement;
          const sourceIds = new Set<string>(videoData.generatedFrom?.sourceIds ?? []);
          if (videoData.startImageId) {
            sourceIds.add(videoData.startImageId);
          }
          sourceIds.add(imageNode.id);
          const updates: Partial<VideoElement> = {
            endImageId: imageNode.id,
            endImageUrl: imageNode.src,
            generatedFrom: {
              type: 'image-to-image',
              sourceIds: Array.from(sourceIds),
              prompt: videoData.promptText,
            },
          };
          if (videoData.status === 'ready') {
            updates.status = 'pending';
            updates.progress = 0;
            updates.src = '';
            updates.thumbnail = '';
          }
          updateElement(targetId, updates);
          handled = true;
        }

        if (!handled) {
          return;
        }

        upsertEdge();
        return;
      }

      // å…¶ä»–èŠ‚ç‚¹è¿æ¥ï¼Œç›´æ¥åˆ›å»ºè¿çº¿
      upsertEdge();
    },
    [elements, setEdges, updateElement]
  );

  // Next Shot Generation Logic
  const handleNextShotGeneration = useCallback(async (sourceNodeId: string, userInstruction?: string, count: number = 1) => {
    const { elements: storeElements, apiConfig, addElement, updateElement, deleteElement } = useCanvasStore.getState();
    const sourceNode = storeElements.find(el => el.id === sourceNodeId) as ImageElement | undefined;

    if (!sourceNode || !sourceNode.src) {
      toast.error('æ‰¾ä¸åˆ°æºå›¾ç‰‡');
      return;
    }

    if (!apiConfig.dashScopeApiKey) {
      toast.error('è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® DashScope API Key (ç”¨äº Qwen VL)');
      // Open settings modal?
      return;
    }

    // 1. Create Placeholder Nodes
    const newImageIds: string[] = [];
    const offset = { x: 450, y: 0 }; // Place to the right
    const size = sourceNode.size || IMAGE_NODE_DEFAULT_SIZE;

    for (let i = 0; i < count; i++) {
      const newImageId = `image-${Date.now()}-${i}-next`;
      newImageIds.push(newImageId);

      const position = {
        x: sourceNode.position.x + offset.x + (i * (size.width + 50)),
        y: sourceNode.position.y, // Horizontal layout
      };

      const placeholderImage: ImageElement = {
        id: newImageId,
        type: 'image',
        position,
        size,
        src: '', // Empty initially
        uploadState: 'syncing', // Loading state
        uploadMessage: count > 1 ? `æ­£åœ¨æ„æ€åˆ†é•œ ${i + 1}/${count}...` : 'æ­£åœ¨æ„æ€ä¸‹ä¸€åˆ†é•œ...',
        generatedFrom: {
          type: 'image-to-image',
          sourceIds: [sourceNode.id],
          prompt: userInstruction || 'Next Shot',
        },
      };

      addElement(placeholderImage);

      // Create Edge (connect to previous node or source node)
      // For sequential shots, maybe connect sequentially? Or all from source?
      // "Next Shot" implies sequence. So Source -> Shot 1 -> Shot 2 -> Shot 3?
      // Or Source -> Shot 1, Source -> Shot 2?
      // User said "Split VL content... call Image-to-Image".
      // Usually "Next Shot" means the *next* shot in time.
      // If we generate 4 shots, are they 4 *options* for the next shot, or a sequence of 4 shots?
      // "Generate corresponding number of shots... split... call Image-to-Image".
      // Assuming they are sequential shots (Shot 1, then Shot 2, etc.) or just 4 distinct shots following the source.
      // Let's connect all to Source for now, as they are all generated *from* the source image context.
      // Or maybe connect sequentially if they are a sequence.
      // Let's stick to connecting all to Source for simplicity in this "Auto Next Shot" context, 
      // as they are all based on the *current* image. 
      // Actually, if it's a "storyboard", they might be sequential.
      // But VL analyzes *this* image to generate *next* shots.
      // Let's connect all to Source.

      const edgeId = `edge-${sourceNode.id}-${newImageId}`;
      setEdges((eds) => [
        ...eds,
        {
          id: edgeId,
          source: sourceNode.id,
          target: newImageId,
          animated: true,
          style: { stroke: '#a855f7', strokeWidth: 2, strokeDasharray: '5,5' },
        },
      ]);
    }

    resetConnectionMenu();

    // 2. Background Process
    (async () => {
      try {
        // A. Qwen VL Analysis
        let imageUrlForApi = sourceNode.src;
        if (sourceNode.base64) {
          imageUrlForApi = sourceNode.base64.startsWith('data:') ? sourceNode.base64 : `data:image/png;base64,${sourceNode.base64}`;
        }

        const systemPrompt = `You are a professional storyboard artist. Analyze this image as "Frame 0" (the starting point).

TASK: Generate ${count} NEW sequential shots that happen AFTER this image. Each prompt describes what happens NEXT, not what's currently shown.

CRITICAL RULES:
1. Frame 1 must show ACTION or CHANGE from Frame 0 - different angle, character movement, time progression, or new element
2. Never describe the current image - only what comes AFTER it
3. Maintain visual consistency: same characters, style, lighting mood, color palette
4. Each shot should advance the narrative or camera position

PROMPT FORMAT: Focus on action + camera + mood. Be concise (under 60 words each).

OUTPUT: Return ONLY a JSON array of ${count} strings. No markdown, no explanation.
Example: ["Medium shot, character turns head toward the door, tension building, same warm lighting", "Close-up of door handle slowly turning, shallow depth of field, suspenseful"]`;
        
        const userPrompt = userInstruction
          ? `Based on this image (Frame 0), generate ${count} prompts for what happens NEXT following this direction: "${userInstruction}". 
Each prompt must describe a NEW shot (not the current image). Maintain visual style consistency.
Return ONLY a JSON array of ${count} strings.`
          : systemPrompt;

        const response = await fetch('https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiConfig.dashScopeApiKey}`
          },
          body: JSON.stringify({
            model: 'qwen3-vl-flash',
            messages: [{
              role: 'user',
              content: [
                { type: 'image_url', image_url: { url: imageUrlForApi } },
                { type: 'text', text: userPrompt }
              ]
            }]
          })
        });

        if (!response.ok) {
          const err = await response.json();
          throw new Error(err.error?.message || 'Qwen VL API request failed');
        }

        const data = await response.json();
        let content = data.choices[0]?.message?.content || '';

        // Clean up markdown code blocks if present
        content = content.replace(/```json\n?|\n?```/g, '').trim();

        let nextShotPrompts: string[] = [];
        try {
          const parsed = JSON.parse(content);
          if (Array.isArray(parsed)) {
            nextShotPrompts = parsed.map(p => String(p));
          } else if (typeof parsed === 'string') {
            nextShotPrompts = [parsed];
          }
        } catch (e) {
          // Fallback: split by newlines or just treat as one prompt if parsing fails
          console.warn('Failed to parse VL response as JSON, using raw text', e);
          nextShotPrompts = [content];
        }

        // Ensure we have enough prompts (duplicate last if needed, or just use what we have)
        // If VL returns fewer prompts than requested, we only generate that many.
        // If VL returns more, we truncate.

        // B. Image Generation (Batch)
        // We need mediaId for image-to-image. If not present, upload it.
        let effectiveMediaId = sourceNode.mediaId || sourceNode.mediaGenerationId;

        if (!effectiveMediaId) {
          // Upload if needed
          if (!sourceNode.base64 && !sourceNode.src.startsWith('data:')) {
            throw new Error('Source image not ready for generation (missing mediaId)');
          }

          // If we have base64, upload it
          const base64 = sourceNode.base64 || sourceNode.src.split(',')[1];
          const { registerUploadedImage } = await import('@/lib/api-mock');
          const uploadResult = await registerUploadedImage(base64);
          effectiveMediaId = uploadResult.mediaGenerationId || undefined;

          // Update source node with new mediaId
          if (effectiveMediaId) {
            updateElement(sourceNode.id, { mediaGenerationId: effectiveMediaId } as Partial<ImageElement>);
          }
        }

        // Ensure we have prompts for all placeholders
        // If VL returned fewer prompts, repeat the last one or use default
        const finalPrompts = newImageIds.map((_, i) => nextShotPrompts[i] || nextShotPrompts[nextShotPrompts.length - 1] || 'Next Shot');

        const { imageToImage } = await import('@/lib/api-mock');
        const result = await imageToImage(
          finalPrompts[0], // Primary prompt (unused if prompts array is provided)
          sourceNode.src,
          '16:9', // Default aspect ratio
          sourceNode.caption || '',
          effectiveMediaId,
          newImageIds.length, // Count
          finalPrompts // Pass all prompts
        );

        // C. Update Placeholders with Final Results
        if (result.images && result.images.length > 0) {
          newImageIds.forEach((imageId, index) => {
            const imgData = result.images![index];
            if (imgData) {
              updateElement(imageId, {
                src: imgData.imageUrl || imgData.fifeUrl,
                base64: imgData.base64,
                promptId: result.promptId, // Share same promptId or generate new?
                mediaId: imgData.mediaId || imgData.mediaGenerationId,
                mediaGenerationId: imgData.mediaGenerationId,
                uploadState: 'synced',
                uploadMessage: undefined,
                generatedFrom: {
                  type: 'image-to-image',
                  sourceIds: [sourceNode.id],
                  prompt: imgData.prompt || finalPrompts[index]
                }
              } as Partial<ImageElement>);

              // Update edge style
              const edgeId = `edge-${sourceNode.id}-${imageId}`;
              setEdges((eds) => eds.map(e => e.id === edgeId ? { ...e, animated: false, style: { stroke: '#64748b', strokeWidth: 1 } } : e));
            }
          });
        } else {
          throw new Error('No images generated');
        }

      } catch (error) {
        console.error('Next shot generation failed:', error);
        toast.error(`ç”Ÿæˆå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
        // Delete all placeholders
        newImageIds.forEach(id => deleteElement(id));
      }
    })();

  }, [resetConnectionMenu, setEdges]);



  const handleAutoNextShot = useCallback((count: number = 1) => {
    if (connectionMenu.sourceNodeId) {
      handleNextShotGeneration(connectionMenu.sourceNodeId, undefined, count);
    }
  }, [connectionMenu.sourceNodeId, handleNextShotGeneration]);

  const handleCustomNextShot = useCallback(() => {
    showCustomNextShotInput();
  }, [showCustomNextShotInput]);

  const handleConfirmCustomNextShot = useCallback(() => {
    if (connectionMenu.sourceNodeId && connectionMenu.pendingImageConfig?.prompt) {
      handleNextShotGeneration(connectionMenu.sourceNodeId, connectionMenu.pendingImageConfig.prompt);
    }
  }, [connectionMenu.sourceNodeId, connectionMenu.pendingImageConfig, handleNextShotGeneration]);

  // è¡Œçº§æ³¨é‡Šï¼šè¡”æ¥é•œå¤´ç”Ÿæˆ - åˆ†æä¸¤å¼ å›¾ç‰‡ï¼Œç”Ÿæˆä¸­é—´è¿‡æ¸¡çš„åˆ†é•œ
  const handleTransitionShotsGeneration = useCallback(async (startImage: ImageElement, endImage: ImageElement) => {
    const { apiConfig, addElement, updateElement, deleteElement } = useCanvasStore.getState();

    // æ£€æŸ¥ DashScope API Key
    if (!apiConfig.dashScopeApiKey) {
      toast.error('è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® DashScope API Key (ç”¨äº Qwen VL)');
      return;
    }

    // æ£€æŸ¥å›¾ç‰‡æ˜¯å¦æœ‰æ•ˆ
    if (!startImage.src || !endImage.src) {
      toast.error('å›¾ç‰‡å†…å®¹æ— æ•ˆ');
      return;
    }

    toast.info('æ­£åœ¨åˆ†æä¸¤å¼ å›¾ç‰‡ï¼Œç”Ÿæˆè¡”æ¥åˆ†é•œ...');

    // 1. è®¡ç®—å ä½èŠ‚ç‚¹ä½ç½®ï¼ˆæ”¾åœ¨ä¸¤å¼ å›¾ä¸­é—´ï¼‰
    const midX = (startImage.position.x + endImage.position.x) / 2;
    const midY = (startImage.position.y + endImage.position.y) / 2;
    const size = startImage.size || IMAGE_NODE_DEFAULT_SIZE;

    // 2. åˆ›å»º 1 ä¸ªå ä½èŠ‚ç‚¹
    const placeholderId = `image-${Date.now()}-transition`;

    const placeholderImage: ImageElement = {
      id: placeholderId,
      type: 'image',
      position: { x: midX, y: midY },
      size,
      src: '',
      uploadState: 'syncing',
      uploadMessage: 'æ­£åœ¨åˆ†æè¡”æ¥é•œå¤´...',
      generatedFrom: {
        type: 'image-to-image',
        sourceIds: [startImage.id, endImage.id],
        prompt: 'è¡”æ¥é•œå¤´',
      },
    };

    addElement(placeholderImage);

    // åˆ›å»ºè¿çº¿ï¼ˆèµ·ç‚¹ â†’ å ä½ â†’ ç»ˆç‚¹ï¼‰
    setEdges((eds) => [
      ...eds,
      {
        id: `edge-${startImage.id}-${placeholderId}`,
        source: startImage.id,
        target: placeholderId,
        animated: true,
        style: { stroke: '#06b6d4', strokeWidth: 2, strokeDasharray: '5,5' },
      },
      {
        id: `edge-${placeholderId}-${endImage.id}`,
        source: placeholderId,
        target: endImage.id,
        animated: true,
        style: { stroke: '#06b6d4', strokeWidth: 2, strokeDasharray: '5,5' },
      },
    ]);

    // 3. åå°å¤„ç†
    (async () => {
      try {
        // A. å‡†å¤‡å›¾ç‰‡ URL
        let startImageUrl = startImage.src;
        let endImageUrl = endImage.src;

        if (startImage.base64) {
          startImageUrl = startImage.base64.startsWith('data:') ? startImage.base64 : `data:image/png;base64,${startImage.base64}`;
        }
        if (endImage.base64) {
          endImageUrl = endImage.base64.startsWith('data:') ? endImage.base64 : `data:image/png;base64,${endImage.base64}`;
        }

        // B. è°ƒç”¨ Qwen VL åˆ†æä¸¤å¼ å›¾ç‰‡ï¼Œç”Ÿæˆ 1 ä¸ªè¡”æ¥åˆ†é•œ
        const systemPrompt = `You are a professional film director.

TASK: Analyze "Frame A" (first image) and "Frame B" (second image).
Generate exactly ONE cinematic transition shot that bridges from A to B.

Identify what changes between the two frames (character, camera, mood, action).
Design a single smooth transition that connects these moments naturally.

OUTPUT: Return ONLY ONE prompt string (not JSON array), under 60 words, in English.
Example: "Medium shot, character mid-turn with motion blur, same warm lighting, tension building as the scene transitions"`;

        const response = await fetch('https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiConfig.dashScopeApiKey}`
          },
          body: JSON.stringify({
            model: 'qwen-vl-max',
            messages: [{
              role: 'user',
              content: [
                { type: 'image_url', image_url: { url: startImageUrl } },
                { type: 'image_url', image_url: { url: endImageUrl } },
                { type: 'text', text: systemPrompt }
              ]
            }]
          })
        });

        if (!response.ok) {
          const err = await response.json();
          throw new Error(err.error?.message || 'Qwen VL API request failed');
        }

        const data = await response.json();
        let transitionPrompt = data.choices[0]?.message?.content || '';

        // æ¸…ç† markdown å’Œå¤šä½™æ ¼å¼
        transitionPrompt = transitionPrompt.replace(/```json\n?|\n?```/g, '').replace(/^["'\[\]]+|["'\[\]]+$/g, '').trim();

        if (!transitionPrompt) {
          throw new Error('AI æœªè¿”å›æœ‰æ•ˆçš„åˆ†é•œæè¿°');
        }

        console.log('ğŸ¬ VL åˆ†æç»“æœ - è¡”æ¥åˆ†é•œ:', transitionPrompt);

        // C. æ›´æ–°å ä½èŠ‚ç‚¹çŠ¶æ€
        updateElement(placeholderId, {
          uploadMessage: 'æ­£åœ¨ç”Ÿæˆè¡”æ¥é•œå¤´...',
          generatedFrom: {
            type: 'image-to-image',
            sourceIds: [startImage.id, endImage.id],
            prompt: transitionPrompt,
          },
        } as Partial<ImageElement>);

        // D. è°ƒç”¨å›¾ç”Ÿå›¾ API ç”Ÿæˆè¡”æ¥é•œå¤´
        let effectiveMediaId = startImage.mediaId || startImage.mediaGenerationId;

        if (!effectiveMediaId) {
          if (!startImage.base64 && !startImage.src.startsWith('data:')) {
            throw new Error('Source image not ready for generation (missing mediaId)');
          }
          const base64 = startImage.base64 || startImage.src.split(',')[1];
          const { registerUploadedImage } = await import('@/lib/api-mock');
          const uploadResult = await registerUploadedImage(base64);
          effectiveMediaId = uploadResult.mediaGenerationId || undefined;
        }

        const { imageToImage } = await import('@/lib/api-mock');
        const result = await imageToImage(
          transitionPrompt,
          startImage.src,
          '16:9',
          startImage.caption || '',
          effectiveMediaId,
          1 // åªç”Ÿæˆ 1 å¼ 
        );

        // E. æ›´æ–°å ä½èŠ‚ç‚¹
        if (result.images && result.images.length > 0) {
          const imgData = result.images[0];
          updateElement(placeholderId, {
            src: imgData.imageUrl || imgData.fifeUrl,
            base64: imgData.base64,
            promptId: result.promptId,
            mediaId: imgData.mediaId || imgData.mediaGenerationId,
            mediaGenerationId: imgData.mediaGenerationId,
            uploadState: 'synced',
            uploadMessage: undefined,
            generatedFrom: {
              type: 'image-to-image',
              sourceIds: [startImage.id, endImage.id],
              prompt: imgData.prompt || transitionPrompt
            }
          } as Partial<ImageElement>);

          // åœæ­¢è¿çº¿åŠ¨ç”»
          setEdges((eds) => eds.map(e => 
            e.target === placeholderId || e.source === placeholderId
              ? { ...e, animated: false, style: { stroke: '#06b6d4', strokeWidth: 2 } }
              : e
          ));

          toast.success('è¡”æ¥é•œå¤´ç”Ÿæˆå®Œæˆ');
        } else {
          throw new Error('No images generated');
        }

      } catch (error) {
        console.error('Transition shots generation failed:', error);
        toast.error(`ç”Ÿæˆå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
        // åˆ é™¤å ä½èŠ‚ç‚¹
        deleteElement(placeholderId);
      }
    })();

  }, [setEdges]);

  return (
    <div ref={reactFlowWrapperRef} className="w-full h-full bg-gray-50 relative">
      {/* é¡¶éƒ¨å¯¼èˆª */}
      <CanvasNavigation />

      {/* å·¦ä¾§å·¥å…·æ  - èŠ‚ç‚¹åˆ›å»º */}
      <Toolbar />

      {/* å³ä¾§å·¥å…·æ  - åŠŸèƒ½æŒ‰é’® */}
      <RightToolbar />

      <ReactFlow
        className="custom-theme absolute inset-0"
        nodes={reactFlowNodes}
        edges={edges}
        onNodesChange={handleNodesChange}
        onEdgesChange={onEdgesChange}
        onNodeDrag={handleNodeDrag}
        onNodeDragStop={handleNodeDragStop}
        onSelectionChange={handleSelectionChange}
        onConnect={handleConnect}
        onConnectStart={handleConnectStart}
        onConnectEnd={handleConnectEnd}
        nodeTypes={nodeTypes}
        fitView
        selectNodesOnDrag={false}
        selectionMode={SelectionMode.Partial}
        selectionOnDrag
        panOnScroll
        panOnDrag={[1, 2]} // ä¸­é”®å’Œå³é”®æ‹–æ‹½
        zoomOnScroll
        minZoom={0.1}
        maxZoom={3}
        defaultViewport={{ x: 0, y: 0, zoom: 1 }}
        nodesDraggable={true}
        nodesConnectable={true}
        elementsSelectable={true}
        connectionLineStyle={{ stroke: '#a855f7', strokeWidth: 2 }}
        connectionLineType={ConnectionLineType.Bezier}
        onlyRenderVisibleElements={true} // è¡Œçº§æ³¨é‡Šï¼šåªæ¸²æŸ“è§†å£å†…çš„èŠ‚ç‚¹ï¼Œå¤§å¹…æå‡æ€§èƒ½
        proOptions={{ hideAttribution: true }}
      >
        {/* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® - å³ä¸‹è§’ */}
        <div className="absolute bottom-4 right-4 z-10">
          <ThemeToggle />
        </div>

        {/* èƒŒæ™¯ç½‘æ ¼ - æ ¹æ®ä¸»é¢˜åˆ‡æ¢é¢œè‰² */}
        {uiState.showGrid && (
          <Background
            variant={BackgroundVariant.Dots}
            gap={40}
            size={4}
            color={theme === 'dark' ? '#334155' : '#dddddd'}
            bgColor={theme === 'dark' ? '#0f172a' : '#f0f0f0'}
          />
        )}

        {/* AI è¾“å…¥é¢æ¿ - æ”¾åœ¨ React Flow å†…éƒ¨ä»¥ä½¿ç”¨ useReactFlow */}
        <AIInputPanel />
      </ReactFlow>

      {/* è¡Œçº§æ³¨é‡Šï¼šè¿çº¿é€‰é¡¹èœå•ç»„ä»¶ */}
      <ConnectionMenuRoot
        state={connectionMenu}
        callbacks={{
          onShowImageSubmenu: showImageSubmenu,
          onShowVideoSubmenu: showVideoSubmenu,
          onGenerateImage: handleGenerateImage,
          onGenerateVideo: handleGenerateVideo,
          onGenerateVideoFromImage: handleGenerateVideoFromImage,
          onImagePromptInputChange: handleImagePromptInputChange,
          onConfirmImagePrompt: handleConfirmImagePrompt,
          onBackToMain: backToMain,
          onBackToImageSubmenu: backToImageSubmenu,
          onClose: resetConnectionMenu,
          onShowCameraControlSubmenu: showCameraControlSubmenu,
          onShowCameraPositionSubmenu: showCameraPositionSubmenu,
          onGenerateReshoot: handleGenerateReshoot,
          onShowExtendVideoSubmenu: handleShowExtendVideo,
          onExtendPromptChange: () => { }, // è¡Œçº§æ³¨é‡Šï¼šä¸å†éœ€è¦ï¼Œä¿ç•™æ¥å£å…¼å®¹æ€§
          onConfirmExtend: () => { }, // è¡Œçº§æ³¨é‡Šï¼šä¸å†éœ€è¦ï¼Œä¿ç•™æ¥å£å…¼å®¹æ€§
          onAutoNextShot: handleAutoNextShot,
          onCustomNextShot: handleCustomNextShot,
          onConfirmCustomNextShot: handleConfirmCustomNextShot,
          onShowAutoNextShotCountSubmenu: showAutoNextShotCountSubmenu,
        }}
        promptInputRef={promptMenuInputRef}
      />

      {/* å¤šé€‰å·¥å…·æ  */}
      <SelectionToolbar 
        onMultiImageEdit={handleMultiImageEdit} 
        onTransitionShots={handleTransitionShotsGeneration}
      />

      {/* å›¾ç‰‡ç¼–è¾‘å™¨ Modal - å…¨å±€æ¸²æŸ“ */}
      <ImageAnnotatorModal
        open={Boolean(annotatorTarget)}
        imageSrc={annotatorTarget?.src || null}
        isLoadingImage={isLoadingAnnotatorImage}
        mainImage={mainImageForEdit}
        referenceImages={referenceImages}
        onClose={handleAnnotatorClose}
        onConfirm={handleAnnotatorConfirm}
      />
    </div>
  );
}

export default function Canvas({ projectId }: { projectId?: string }) {
  return (
    <ReactFlowProvider>
      <CanvasContent projectId={projectId} />
    </ReactFlowProvider>
  );
}